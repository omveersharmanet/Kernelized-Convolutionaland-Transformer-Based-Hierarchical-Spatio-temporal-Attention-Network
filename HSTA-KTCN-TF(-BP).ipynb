{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fa80c9a",
   "metadata": {},
   "source": [
    "#  Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07fef787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version: 3.8.18 (default, Sep 11 2023, 13:39:12) [MSC v.1916 64 bit (AMD64)]\n",
      "Torch Version: 2.1.0\n",
      "Matplotlib Version: 3.5.3\n",
      "Numpy Version: 1.24.3\n",
      "sklearn Version: 1.1.3\n",
      "Pandas Version: 1.5.3\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "import time,os,sys,datetime\n",
    "\n",
    "import IPython\n",
    "import IPython.display\n",
    "\n",
    "#Plotting tools\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "#import seaborn as sns\n",
    "plt.style.use('ggplot')\n",
    "rcParams['figure.figsize'] = (14, 14)\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "%matplotlib inline\n",
    "import numpy\n",
    "#Computation tools\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "from six import iteritems\n",
    "import sklearn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import mat73\n",
    "import scipy.io as sio\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from pickle import dump\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader \n",
    "import torch.nn.functional as F\n",
    "#Printing out the Versions of libraries \n",
    "print('Python Version: {}'.format(sys.version))\n",
    "print('Torch Version: {}'.format(torch.__version__))\n",
    "print('Matplotlib Version: {}'.format(mpl.__version__))\n",
    "print('Numpy Version: {}'.format(np.__version__))\n",
    "print('sklearn Version: {}'.format(sklearn.__version__))\n",
    "print('Pandas Version: {}'.format(pd.__version__))\n",
    "#print('Seaborn Version: {}'.format(sns.__version__))\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import joblib\n",
    "#from sklearn.externals import joblib\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d25e9532",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_length = 15\n",
    "decoder_length=26\n",
    "input_length=encoder_length+decoder_length\n",
    "batch_size = 64\n",
    "stride_trining=decoder_length-1\n",
    "stride_testing=decoder_length-1\n",
    "copyies=1\n",
    "no_of_epoch = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780230ae",
   "metadata": {},
   "source": [
    "# Data loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e7a077d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = mat73.loadmat(r'C:\\Users\\user\\Downloads\\New folder (2)\\review\\mypapers\\IJIT_KTCN\\R1\\Repos\\NGSIM_test_data.mat')\n",
    "data_x_train = data_dict['train_data']\n",
    "data_x_test = data_dict['Test_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a63298e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = sio.loadmat(r'C:\\Users\\user\\Downloads\\New folder (2)\\review\\mypapers\\IJIT_KTCN\\R1\\Repos\\NGSIM_test_data.mat')\n",
    "#data_x_train = data_dict['train_data']\n",
    "data_x_test = data_dict['Test_data']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bd2d68",
   "metadata": {},
   "source": [
    "# Actual Trajectory Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f8127767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 600.0)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x1400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_features = data_x_test[1900][0][:,1:3]\n",
    "plot_features =pd.DataFrame(plot_features)\n",
    "#plot_features = dataset_fridge['fridge'][155000 : 156500]\n",
    "#_ = plot_features.plot(subplots=False)\n",
    "plt.plot(plot_features[0],plot_features[1])\n",
    "plt.xlim(0, 60)\n",
    "plt.ylim(0, 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f2dcca",
   "metadata": {},
   "source": [
    "## Timeseries making"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4277c490",
   "metadata": {},
   "outputs": [],
   "source": [
    "class timeseries(Dataset):\n",
    "    def __init__(self,x,y,z):\n",
    "        self.x = torch.tensor(x,dtype=torch.float32)\n",
    "        self.y = torch.tensor(y,dtype=torch.float32)\n",
    "        self.z = torch.tensor(z,dtype=torch.float32)        \n",
    "        self.len = x.shape[0]\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return self.x[idx],self.y[idx],self.z[idx]\n",
    "  \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "#train_x_data_tensor_3d=train_x_data_tensor_3d.numpy()\n",
    "#train_x_data_tensor_3d=train_x_data_tensor_3d.repeat(1,1,copyies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dcbbc0",
   "metadata": {},
   "source": [
    "### Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0d4067b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AllLoss(nn.Module):\n",
    "    def __init__(self, eps=1e-7):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        \n",
    "        self.mse = nn.MSELoss()\n",
    "        self.mae=nn.L1Loss()\n",
    "        self.KLD=nn.KLDivLoss(reduction=\"batchmean\")\n",
    "        \n",
    "    def rmse(self,a,b):\n",
    "        return torch.sqrt(self.mse(a,b) + self.eps)  \n",
    "\n",
    "        \n",
    "    def forward(self,yhat,y):\n",
    "        #RMSE_loss = torch.sqrt(self.mse(yhat,y) + self.eps)\n",
    "        RMSE_loss = self.rmse(yhat,y)\n",
    "        MSE_Loss= self.mse(yhat,y)\n",
    "        MAE_Loss=self.mae(yhat,y)\n",
    "        #KLD_Loss_softmax=self.KLD(F.softmax(yhat),F.softmax(y))\n",
    "        L1=torch.sqrt(self.mse(yhat[:,:,0],y[:,:,0]) + self.eps)\n",
    "        L2=torch.sqrt(self.mse(yhat[:,:,1],y[:,:,1]) + self.eps) \n",
    "        KLD_Loss=self.KLD(yhat,y)\n",
    "        return [1.41*RMSE_loss,2*MSE_Loss,2*MAE_Loss,L1,L2,KLD_Loss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3131fff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn1=nn.CrossEntropyLoss()\n",
    "loss_fn = AllLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424bf392",
   "metadata": {},
   "source": [
    "### Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea467bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim_model, dropout_p, max_len):\n",
    "        super().__init__()\n",
    "        # Modified version from: https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "        # max_len determines how far the position can have an effect on a token (window)\n",
    "        \n",
    "        # Info\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "        # Encoding - From formula\n",
    "        pos_encoding = torch.zeros(max_len, dim_model)\n",
    "        positions_list = torch.arange(0, max_len, dtype=torch.float).view(-1, 1) # 0, 1, 2, 3, 4, 5\n",
    "        division_term = torch.exp(torch.arange(0, dim_model, 2).float() * (-math.log(10000.0)) / dim_model) # 1000^(2i/dim_model)\n",
    "        \n",
    "        # PE(pos, 2i) = sin(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 0::2] = torch.sin(positions_list * division_term)\n",
    "        \n",
    "        # PE(pos, 2i + 1) = cos(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 1::2] = torch.cos(positions_list * division_term)\n",
    "        \n",
    "        # Saving buffer (same as parameter without gradients needed)\n",
    "        pos_encoding = pos_encoding.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer(\"pos_encoding\",pos_encoding)\n",
    "        \n",
    "    def forward(self, token_embedding: torch.tensor) -> torch.tensor:\n",
    "        # Residual connection + pos encoding\n",
    "        return self.dropout(token_embedding + self.pos_encoding[:token_embedding.size(0), :])\n",
    "        #return  self.pos_encoding[:token_embedding.size(0), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f4a0c7",
   "metadata": {},
   "source": [
    "# Model for reframing data by host vehicle taking 6 sourrounding vehicle and different mixed teacher training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3858a22a",
   "metadata": {},
   "source": [
    "##  with out normalization of data x,y cordinate changed (host vehicle cordinate are frame of reference)and 3D tensor making train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5b7df37",
   "metadata": {},
   "outputs": [],
   "source": [
    "stride_trining_new=5\n",
    "stride_testing_new=5\n",
    "avg_l_x=10\n",
    "avg_l_v=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "def3c623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are many features in the NGSIM dataset, here, only positional features have been taken \n",
    "training_features=[0,77,78,79,80,81,82,1,89,90,91,92,93,94,2,3,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c59e61f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_features=  [0,1]\n",
    "testing_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1be97bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Raw_x_training_data= []      \n",
    "Raw_y_training_data= []  \n",
    "Raw_z_training_data= []  \n",
    "for i in range(0,len(data_x_train)):\n",
    "    x = torch.tensor(data_x_train[i][0][:,training_features]).clone().detach()\n",
    "    y = torch.tensor(data_x_train[i][0][:,testing_features]).clone().detach()    \n",
    "    x= x[::2].clone().detach()\n",
    "    y= y[::2].clone().detach()\n",
    "    \n",
    "    if len(x)>input_length:\n",
    "        for j in range(0,len(x)+1-encoder_length-decoder_length-max(avg_l_x,avg_l_v),stride_trining_new):\n",
    "            a=x[j:j+encoder_length][:].clone().detach()\n",
    "            b=y[j+encoder_length:j+encoder_length+decoder_length][:].clone().detach()\n",
    "            d_new= torch.zeros(b.shape[0], 2)\n",
    "            for k in range (0, b.shape[0]):\n",
    "                d=[]\n",
    "                if torch.mean(x[j+encoder_length+k : j+encoder_length+k+avg_l_x,14])<-0.2:\n",
    "                    if 0.8*x[j+encoder_length+k-1,15]> torch.mean(x[j+encoder_length+k:j+encoder_length+k+avg_l_v,15]):\n",
    "                        d=[0,0]\n",
    "                    elif 1.2*x[j+encoder_length+k-1,15]< torch.mean(x[j+encoder_length+k:j+encoder_length+k+avg_l_v,15]):\n",
    "                        d=[0,2]\n",
    "                    else:\n",
    "                        d=[0,1]\n",
    "                            \n",
    "                elif torch.mean(x[j+encoder_length+k : j+encoder_length+k+avg_l_x,14])>0.2:\n",
    "                    \n",
    "                    if 0.8*x[j+encoder_length+k-1,15]> torch.mean(x[j+encoder_length+k : j+encoder_length+k+avg_l_v,15]):\n",
    "                        d=[2,0]\n",
    "                    elif 1.2*x[j+encoder_length+k-1,15]< torch.mean(x[j+encoder_length+k:j+encoder_length+k+avg_l_v,15]):\n",
    "                        d=[2,2]\n",
    "                    else:\n",
    "                        d=[2,1]      \n",
    "                else:\n",
    "                    if 0.8*x[j+encoder_length+k-1,15]> torch.mean(x[j+encoder_length+k:j+encoder_length+k+avg_l_v,15]):\n",
    "                        d=[1,0]\n",
    "                    elif 1.2*x[j+encoder_length+k-1,15]< torch.mean(x[j+encoder_length+k:j+encoder_length+k+avg_l_v,15]):\n",
    "                        d=[1,2]\n",
    "                    else:\n",
    "                        d=[1,1]    \n",
    "                #print(d)\n",
    "                d_new[k,:]=torch.tensor(d).clone().detach()\n",
    "            del d\n",
    "            c=a[-1,[0,7]].clone().detach()\n",
    "            \n",
    "            #a[:,0:2]=a[:,0:2]-c   \n",
    "            a[:,0:7]=a[:,0:7]-c[0] \n",
    "            a[:,7:14]=a[:,7:14]-c[1]  \n",
    "            b[:,0:2]=b[:,0:2]-c\n",
    "            \n",
    "            Raw_x_training_data.append(a.clone().detach())\n",
    "            Raw_y_training_data.append(b.clone().detach())\n",
    "            Raw_z_training_data.append(d_new.clone().detach())\n",
    "            del a,b,c\n",
    "        #print(d_new)    \n",
    "    del x,y,d_new\n",
    "    \n",
    "Raw_x_training_data = torch.stack(Raw_x_training_data, 0)            \n",
    "Raw_y_training_data = torch.stack(Raw_y_training_data, 0) \n",
    "Raw_z_training_data = torch.stack(Raw_z_training_data, 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07a54a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "Raw_x_testing_data= []      \n",
    "Raw_y_testing_data= []  \n",
    "Raw_z_testing_data= []  \n",
    "for i in range(0,len(data_x_test)):\n",
    "#for i in range(0,20):\n",
    "    #x = torch.tensor(data_x_test[i][0][:,0:training_features]).clone().detach()\n",
    "    #y = torch.tensor(data_x_test[i][0][:,0:testing_features]).clone().detach()\n",
    "    x = torch.tensor(data_x_test[i][0][:,training_features]).clone().detach()\n",
    "    y = torch.tensor(data_x_test[i][0][:,testing_features]).clone().detach()\n",
    "    \n",
    "    x= x[::2].clone().detach()\n",
    "    y= y[::2].clone().detach()\n",
    "    if len(x)>input_length:\n",
    "        for j in range(0,len(x)+1-encoder_length-decoder_length-max(avg_l_x,avg_l_v),stride_trining_new):\n",
    "            a=x[j:j+encoder_length][:].clone().detach()\n",
    "            b=y[j+encoder_length:j+encoder_length+decoder_length][:].clone().detach()\n",
    "            d_new= torch.zeros(b.shape[0], 2)\n",
    "            for k in range (0, b.shape[0]):\n",
    "                d=[]\n",
    "                if torch.mean(x[j+encoder_length+k : j+encoder_length+k+avg_l_x,14])<-0.2:\n",
    "                    if 0.8*x[j+encoder_length+k-1,15]> torch.mean(x[j+encoder_length+k:j+encoder_length+k+avg_l_v,15]):\n",
    "                        d=[0,0]\n",
    "                    elif 1.2*x[j+encoder_length+k-1,15]< torch.mean(x[j+encoder_length+k:j+encoder_length+k+avg_l_v,15]):\n",
    "                        d=[0,2]\n",
    "                    else:\n",
    "                        d=[0,1]\n",
    "                            \n",
    "                elif torch.mean(x[j+encoder_length+k : j+encoder_length+k+avg_l_x,14])>0.2:\n",
    "                    \n",
    "                    if 0.8*x[j+encoder_length+k-1,15]> torch.mean(x[j+encoder_length+k : j+encoder_length+k+avg_l_v,15]):\n",
    "                        d=[2,0]\n",
    "                    elif 1.2*x[j+encoder_length+k-1,15]< torch.mean(x[j+encoder_length+k:j+encoder_length+k+avg_l_v,15]):\n",
    "                        d=[2,2]\n",
    "                    else:\n",
    "                        d=[2,1]      \n",
    "                else:\n",
    "                    if 0.8*x[j+encoder_length+k-1,15]> torch.mean(x[j+encoder_length+k:j+encoder_length+k+avg_l_v,15]):\n",
    "                        d=[1,0]\n",
    "                    elif 1.2*x[j+encoder_length+k-1,15]< torch.mean(x[j+encoder_length+k:j+encoder_length+k+avg_l_v,15]):\n",
    "                        d=[1,2]\n",
    "                    else:\n",
    "                        d=[1,1]    \n",
    "                #print(d)\n",
    "                d_new[k,:]=torch.tensor(d).clone().detach()\n",
    "            del d\n",
    "            \n",
    "            c=a[-1,[0,7]].clone().detach()\n",
    "            #a[:,0:7]=a[:,0:7]-c       \n",
    "            a[:,0:7]=a[:,0:7]-c[0] \n",
    "            a[:,7:14]=a[:,7:14]-c[1]  \n",
    "            b[:,0:2]=b[:,0:2]-c            \n",
    "            \n",
    "            Raw_x_testing_data.append(a.clone().detach())            \n",
    "            Raw_y_testing_data.append(b.clone().detach())\n",
    "            Raw_z_testing_data.append(d_new.clone().detach())\n",
    "            del a,b,c\n",
    "    del x,y,d_new\n",
    "    \n",
    "Raw_x_testing_data = torch.stack(Raw_x_testing_data, 0)            \n",
    "Raw_y_testing_data = torch.stack(Raw_y_testing_data, 0)\n",
    "Raw_z_testing_data = torch.stack(Raw_z_testing_data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eda19ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "del data_dict,data_x_train ,data_x_test "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4f4b09",
   "metadata": {},
   "source": [
    "##   training and testing  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a85a5c",
   "metadata": {},
   "source": [
    "## Teacher forcing inside sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d606caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop_new(epoch,model, opt,loss_fn1,loss_fn,dataloader,teacher_forcing_ratio):\n",
    "    model.train()\n",
    "    total_loss=0.\n",
    "    cur_loss=0.\n",
    "\n",
    "    for batch,data in enumerate(dataloader):      \n",
    "        current_lr = scheduler.get_lr()[0]\n",
    "        X=data[0].clone().detach()\n",
    "        Y=data[1].clone().detach()\n",
    "        Z=data[2].clone().detach()\n",
    "        #print(X.shape,Y.shape,Z.shape)\n",
    "        Encoder_input=X.clone().detach().to(device) \n",
    "        Decoder_input=Y[:,:-1,:].clone().detach().to(device) \n",
    "        Decoder_output=Y[:,1:,:].clone().detach().to(device) \n",
    "        Encoder_input=torch.tensor(Encoder_input, dtype=torch.float32).to(device)   \n",
    "        Encoder_output=Z[:,:,:].clone().detach().to(device)\n",
    "        Encoder_output = torch.tensor(Encoder_output,dtype=torch.long)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        #if random.random() < teacher_forcing_ratio:   \n",
    "        if teacher_forcing_ratio==1.0:\n",
    "            sequence_length = Decoder_input.shape[1]\n",
    "            tgt_mask = model.get_tgt_mask(sequence_length).to(device) \n",
    "            Decoder_input=Y[:,:-1,:].clone().detach().to(device) \n",
    "            #Decoder_input=torch.cat((Decoder_input,Z[:,:-1,:].clone().detach().to(device) ), dim=2)\n",
    "            Decoder_input=torch.tensor(Decoder_input, dtype=torch.float32).to(device)\n",
    "            tgt_mask=torch.tensor(tgt_mask, dtype=torch.float32).to(device)  \n",
    "            #opt.zero_grad()  \n",
    "        \n",
    "            pred = model(Encoder_input,Decoder_input, tgt_mask)\n",
    "\n",
    "            del sequence_length,tgt_mask,Decoder_input\n",
    "        else:\n",
    "            del Decoder_input\n",
    "            #opt.zero_grad()\n",
    "            SOS_token = Y[:,0,:].clone().detach().to(device) \n",
    "            #SOS_token2 = Z[:,0,:].clone().detach().to(device) \n",
    "            Decoder_input = SOS_token.unsqueeze(1).to(device)  \n",
    "            #Decoder_input=torch.cat((Decoder_input,SOS_token2.unsqueeze(1).to(device)), dim=2)\n",
    "            max_length=Decoder_output.shape[1]\n",
    "            #loss_real = torch.empty(size=(max_length, 6))            \n",
    "            #print(batch,'gello')\n",
    "            sequence_length = Decoder_input.shape[1]\n",
    "            tgt_mask = model.get_tgt_mask(sequence_length).to(device)  \n",
    "            Decoder_input=torch.tensor(Decoder_input, dtype=torch.float32).to(device)\n",
    "            tgt_mask=torch.tensor(tgt_mask, dtype=torch.float32).to(device)\n",
    "            pred1 = model(Encoder_input,Decoder_input, tgt_mask)\n",
    "            \n",
    "            del sequence_length,tgt_mask#,Decoder_input#,loss\n",
    "\n",
    "            for i in range(1,max_length-1):\n",
    "                #print(i)\n",
    "                SOS_token = Y[:,0,:].clone().detach().to(device)\n",
    "                \n",
    "                if random.random() < teacher_forcing_ratio:   \n",
    "                    Decoder_input = torch.cat(  (Decoder_input, Y[:,i,:].unsqueeze(1).to(device) ), dim=1)\n",
    "                else:\n",
    "                    Decoder_input = torch.cat(   (Decoder_input, pred1[:,-1,:].unsqueeze(1)), dim=1)\n",
    "                \n",
    "                \n",
    "                #Decoder_input = torch.cat((Decoder_input,Z[:,0:i+1,:].clone().detach().to(device)), dim=2)\n",
    "                sequence_length = Decoder_input.shape[1]\n",
    "                tgt_mask = model.get_tgt_mask(sequence_length).to(device)  \n",
    "                Decoder_input=torch.tensor(Decoder_input, dtype=torch.float32).to(device)\n",
    "                tgt_mask=torch.tensor(tgt_mask, dtype=torch.float32).to(device)\n",
    "                #print(Encoder_input.shape,Decoder_input.shape, tgt_mask.shape)\n",
    "                pred1 = model(Encoder_input,Decoder_input, tgt_mask) \n",
    "                \n",
    "\n",
    "\n",
    "                del sequence_length,tgt_mask#,a1,a2,Encoder_output_for_loss#,loss,loss1,loss2,loss3\n",
    "            \n",
    "            \n",
    "            if random.random() < teacher_forcing_ratio:   \n",
    "                Decoder_input = torch.cat(  (Decoder_input, Y[:,max_length-1,:].unsqueeze(1).to(device) ), dim=1)\n",
    "            else:\n",
    "                Decoder_input = torch.cat(   (Decoder_input, pred1[:,-1,:].unsqueeze(1)), dim=1)\n",
    "                     \n",
    "            \n",
    "            \n",
    "            \n",
    "           ## Decoder_input = torch.cat((SOS_token.unsqueeze(1), pred1), dim=1)    \n",
    "            #Decoder_input = torch.cat((Decoder_input,Z[:,0:-1,:].clone().detach().to(device)), dim=2)\n",
    "                \n",
    "            sequence_length = Decoder_input.shape[1]\n",
    "            tgt_mask = model.get_tgt_mask(sequence_length).to(device)  \n",
    "            Decoder_input=torch.tensor(Decoder_input, dtype=torch.float32).to(device)\n",
    "            tgt_mask=torch.tensor(tgt_mask, dtype=torch.float32).to(device)        \n",
    "               \n",
    "            pred = model(Encoder_input,Decoder_input, tgt_mask)\n",
    "            #print(Encoder_input.shape,Decoder_input.shape)\n",
    "            del SOS_token,sequence_length,tgt_mask,Decoder_input   \n",
    "        #print(pred.shape, Decoder_output.shape)    \n",
    "        loss = loss_fn.rmse(pred, Decoder_output[:,0:pred.shape[1],:])\n",
    "\n",
    "        #print(loss.shape)\n",
    "        #opt.zero_grad()\n",
    "        loss.backward()\n",
    "        #loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.7)\n",
    "        opt.step()   \n",
    "        opt.zero_grad()\n",
    "        cur_loss += loss.detach().item()\n",
    "\n",
    "        \n",
    "        total_loss += loss.detach().item()\n",
    "        \n",
    "        \n",
    "        if batch % 500 == 0 and batch > 0:\n",
    "            print('tr','|epoch {:3d}|'.format(epoch+1),\n",
    "                  '|lr {:02.6f}|'.format(scheduler.get_lr()[0]),\n",
    "                  '|{:5d}/{:5d} batches|'.format(batch,len(dataloader)),\n",
    "                  'loss {:8.6f}|'.format(loss),\n",
    "                  'loss per 500 batch {:8.3f}|'.format(cur_loss))        \n",
    "            cur_loss = 0.\n",
    "\n",
    "        del pred,loss,X,Y,Encoder_input,Decoder_output#,loss1,loss2,loss3,a1,a2,Encoder_output_for_loss\n",
    "                \n",
    "    scheduler.step()   \n",
    "    return total_loss / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c5fb1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_validation_loop(model,loss_fn1, loss_fn,dataloader,max_length):\n",
    "    #model.eval()\n",
    "    \n",
    "    ##loss_real_list=[]\n",
    "    loss_avg_real= torch.zeros(max_length, 8)\n",
    "    #print(loss_avg_real)\n",
    "    with torch.no_grad():\n",
    "        for batch,data in enumerate(dataloader):     \n",
    "            #print(batch)\n",
    "            X=data[0].clone().detach()\n",
    "            Y=data[1].clone().detach()\n",
    "            Z=data[2].clone().detach()\n",
    "            #print(X.shape,Y.shape,Z.shape)\n",
    "            Encoder_input=X.clone().detach().to(device) \n",
    "            #Decoder_input=Y[:,:-1,:].clone().detach().to(device) \n",
    "            Decoder_output=Y[:,1:,:].clone().detach().to(device) \n",
    "            Encoder_input=torch.tensor(Encoder_input, dtype=torch.float32).to(device)   \n",
    "            Encoder_output=Z[:,:,:].clone().detach().to(device)\n",
    "            Encoder_output = torch.tensor(Encoder_output,dtype=torch.long)\n",
    "        \n",
    "            SOS_token = Y[:,0,:].clone().detach().to(device) \n",
    "            Decoder_input = SOS_token.unsqueeze(1).to(device)  \n",
    "            max_length=Decoder_output.shape[1]\n",
    "            loss_real = torch.zeros(max_length, 8)\n",
    "            \n",
    "            sequence_length = Decoder_input.shape[1]\n",
    "            tgt_mask = model.get_tgt_mask(sequence_length).to(device)  \n",
    "            Decoder_input=torch.tensor(Decoder_input, dtype=torch.float32).to(device)\n",
    "            tgt_mask=torch.tensor(tgt_mask, dtype=torch.float32).to(device)\n",
    "            \n",
    "            pred1 = model(Encoder_input,Decoder_input, tgt_mask)\n",
    "            \n",
    "            \n",
    "            loss = loss_fn(pred1, Decoder_output[:,0:pred1.shape[1],:])  \n",
    "                \n",
    "            loss = torch.Tensor(loss) \n",
    "            #print(loss.shape[1])\n",
    "            loss_real[0,0:6] = loss.clone().detach()       \n",
    "\n",
    "            del sequence_length,tgt_mask,Decoder_input,loss#,loss1,loss2,loss3,a1,a2,Encoder_output_for_loss    \n",
    "            #print(batch,'gello')\n",
    "            for i in range(1,max_length):\n",
    "                Decoder_input = torch.cat((SOS_token.unsqueeze(1), pred1), dim=1)\n",
    "                sequence_length = Decoder_input.shape[1]\n",
    "                tgt_mask = model.get_tgt_mask(sequence_length).to(device)  \n",
    "                \n",
    "                Decoder_input=torch.tensor(Decoder_input, dtype=torch.float32).to(device)\n",
    "                tgt_mask=torch.tensor(tgt_mask, dtype=torch.float32).to(device)\n",
    "                \n",
    "                pred1= model(Encoder_input,Decoder_input, tgt_mask)      \n",
    "\n",
    "            \n",
    "                loss = loss_fn(pred1, Decoder_output[:,0:pred1.shape[1],:])  \n",
    "                \n",
    "                loss = torch.Tensor(loss) \n",
    "                loss_real[i,0:6] = loss.clone().detach()\n",
    "\n",
    "                #loss_real[i] = loss.clone().detach()     \n",
    "                #print(loss_real.shape,loss.shape)\n",
    "                #print('hello')\n",
    "                #print(loss_real[i],loss,loss_real)\n",
    "                #print('sello')\n",
    "                del Decoder_input,loss#,loss1,loss2,loss3,a1,a2,Encoder_output_for_loss                       \n",
    "\n",
    "                \n",
    "            ##loss_real_list+= [loss_real.clone().detach()]\n",
    "            loss_avg_real+= loss_real.clone().detach()\n",
    "            \n",
    "            #print(loss_real.shape,loss_avg_real.shape,len(loss_real_list))\n",
    "            del loss_real,pred1   \n",
    "            #print(loss_avg_real)\n",
    "            #print('tello')\n",
    "    return loss_avg_real/ len(dataloader)#,loss_real_list\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "77181452",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_src_features = Raw_x_testing_data.shape[2]\n",
    "num_of_tgt_features = Raw_y_testing_data.shape[2]\n",
    "num_of_tgt_features2= Raw_z_testing_data.shape[2]\n",
    "#del all_dataloader,val_dataloader\n",
    "\n",
    "t_dataset = timeseries(Raw_x_training_data.repeat(1,1,copyies).numpy(),Raw_y_training_datan.numpy(),Raw_z_training_datan.numpy())\n",
    "v_dataset = timeseries(Raw_x_testing_data.repeat(1,1,copyies).numpy(),Raw_y_testing_data.numpy(),Raw_z_testing_data.numpy())\n",
    "\n",
    "train_dataloader = DataLoader(t_dataset,shuffle=True,batch_size=batch_size)   \n",
    "val_dataloader = DataLoader(v_dataset,shuffle=False,batch_size=batch_size) \n",
    "\n",
    "#del Raw_x_training_data,Raw_y_training_data,Raw_z_training_data,Z_train,t_dataset\n",
    "#del Raw_x_testing_data,Raw_y_testing_data,Raw_z_testing_data,Z_test,v_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e3aa09e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length=Raw_y_testing_data.shape[1] -1\n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "90af3bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "del t_dataset,v_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "103600e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "del  Raw_x_training_data,Raw_y_training_data,Raw_z_training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fccaaad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "del Raw_x_testing_data, Raw_y_testing_data, Raw_z_testing_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b781545e",
   "metadata": {},
   "source": [
    "## Model HSTA-KCTN-TF(-BP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "05c28a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Kerv1d(nn.Conv1d):\n",
    "    r\"\"\"Applies a 1D kervolution over an input signal composed of several input\n",
    "        planes.\n",
    "        Args:\n",
    "            in_channels (int): Number of channels in the input image\n",
    "            out_channels (int): Number of channels produced by the convolution\n",
    "            kernel_size (int or tuple): Size of the convolving kernel\n",
    "            stride (int or tuple, optional): Stride of the convolution. Default: 1\n",
    "            padding (int or tuple, optional): Zero-padding added to both sides of\n",
    "                the input. Default: 0\n",
    "            padding_mode (string, optional). Accepted values `zeros` and `circular` Default: `zeros`\n",
    "            dilation (int or tuple, optional): Spacing between kernel\n",
    "                elements. Default: 1\n",
    "            groups (int, optional): Number of blocked connections from input\n",
    "                channels to output channels. Default: 1\n",
    "            bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``\n",
    "            kernel_type (str), Default: 'linear'\n",
    "            learnable_kernel (bool): Learnable kernel parameters.  Default: False \n",
    "            balance=1, power=3, gamma=1\n",
    "        Shape:\n",
    "            - Input: :math:`(N, C_{in}, L_{in})`\n",
    "            - Output: :math:`(N, C_{out}, L_{out})` where\n",
    "            .. math::\n",
    "                L_{out} = \\left\\lfloor\\frac{L_{in} + 2 \\times \\text{padding} - \\text{dilation}\n",
    "                            \\times (\\text{kernel\\_size} - 1) - 1}{\\text{stride}} + 1\\right\\rfloor\n",
    "        Examples::\n",
    "            >>> m = Kerv1d(16, 33, 3, kernel_type='polynomial', learnable_kernel=True)\n",
    "            >>> input = torch.randn(20, 16, 50)\n",
    "            >>> output = m(input)\n",
    "        .. _kervolution:\n",
    "            https://arxiv.org/pdf/1904.03955.pdf\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, \n",
    "            stride=1, padding=0, dilation=2, groups=1, bias=True, padding_mode='zeros',\n",
    "            kernel_type='linear', learnable_kernel=False, balance=1, power=3, gamma=1):\n",
    "\n",
    "        super(Kerv1d, self).__init__(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias, padding_mode)\n",
    "        self.kernel_type, self.learnable_kernel = kernel_type, learnable_kernel\n",
    "        self.balance, self.power, self.gamma = balance, power, gamma\n",
    "        self.unfold = nn.Unfold((kernel_size,1), (dilation,1), (padding, 0), (stride,1))\n",
    "\n",
    "        # parameter for kernels\n",
    "        if learnable_kernel == True:\n",
    "            self.balance = nn.Parameter(torch.FloatTensor([balance] * out_channels)).view(-1, 1).to(device)\n",
    "            self.gamma   = nn.Parameter(torch.FloatTensor([gamma]   * out_channels)).view(-1, 1).to(device)\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = self.unfold(input.unsqueeze(-1)).unsqueeze(1)\n",
    "        weight  = self.weight.view(self.out_channels, -1, 1)\n",
    "\n",
    "        if self.kernel_type == 'linear':\n",
    "            output = (input * weight).sum(dim=2)\n",
    "\n",
    "        elif self.kernel_type == 'manhattan':\n",
    "            output = -((input - weight).abs().sum(dim=2))\n",
    "\n",
    "        elif self.kernel_type == 'euclidean':\n",
    "            output = -(((input - weight)**2).sum(dim=2))\n",
    "\n",
    "        elif self.kernel_type == 'polynomial':\n",
    "            output = ((input * weight).sum(dim=2) + self.balance)**self.power\n",
    "\n",
    "        elif self.kernel_type == 'gaussian':\n",
    "            output = (-self.gamma*((input - weight)**2).sum(dim=2)).exp() + 0\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError(self.kernel_type+' Kerv1d not implemented')\n",
    "\n",
    "        if self.bias is not None:\n",
    "            output += self.bias.view(self.out_channels, -1)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "    def cuda(self, device=None):\n",
    "        if self.learnable_kernel == True:\n",
    "            self.balance = self.balance.cuda(device)\n",
    "            self.gamma = self.gamma.cuda(device)\n",
    "        return self._apply(lambda t: t.cuda(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef5b624",
   "metadata": {},
   "source": [
    "## KTCN based encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ffd4a899",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KTCN_encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, dim_model, num_of_src_features):\n",
    "        \n",
    "        super(KTCN_encoder, self).__init__()\n",
    "        \n",
    "        #self.dim_model = dim_model\n",
    "        self.num_of_src_features = num_of_src_features\n",
    "        self.num_of_tgt_features = num_of_tgt_features\n",
    "\n",
    "        self.GeLU = nn.GELU()    \n",
    "            \n",
    "        layer_1_dilation_size = 1\n",
    "        layer_1_kernel_size = 2\n",
    "        #self.layer_1_dilation_size = layer_1_dilation_size\n",
    "        #self.layer_1_kernel_size = layer_1_kernel_size\n",
    "        \n",
    "        self.layer_1_num_zero_pad = layer_1_dilation_size*(layer_1_kernel_size-1)\n",
    "        self.temp_conv_layer_1 = nn.Sequential(\n",
    "                        nn.ZeroPad2d((self.layer_1_num_zero_pad, 0, 0, 0)),\n",
    "                        Kerv1d(in_channels = num_of_src_features,\n",
    "                               out_channels = num_of_src_features,\n",
    "                               kernel_size = layer_1_kernel_size,\n",
    "                               stride = 1,\n",
    "                               padding = 0,\n",
    "                               dilation = layer_1_dilation_size,\n",
    "                               kernel_type = 'gaussian', \n",
    "                               learnable_kernel = True).to(device),\n",
    "                        nn.GELU()\n",
    "        )  \n",
    "        \n",
    "        layer_2_dilation_size = 2\n",
    "        layer_2_kernel_size = 2\n",
    "        self.layer_2_num_zero_pad = layer_2_dilation_size*(layer_2_kernel_size-1)\n",
    "        self.temp_conv_layer_2 = nn.Sequential(\n",
    "                        nn.ZeroPad2d((self.layer_2_num_zero_pad, 0, 0, 0)),\n",
    "                        Kerv1d(in_channels = 2*num_of_src_features,\n",
    "                               out_channels = 2*num_of_src_features,\n",
    "                               kernel_size = layer_2_kernel_size,\n",
    "                               stride = 1,\n",
    "                               padding = 0,\n",
    "                               dilation = layer_2_dilation_size,kernel_type = 'gaussian', \n",
    "                               learnable_kernel = True).to(device),\n",
    "                        nn.GELU()\n",
    "        )         \n",
    "        \n",
    "        layer_3_dilation_size = 4\n",
    "        layer_3_kernel_size = 2\n",
    "        self.layer_3_num_zero_pad = layer_3_dilation_size*(layer_3_kernel_size-1)\n",
    "        self.temp_conv_layer_3 = nn.Sequential(\n",
    "                        nn.ZeroPad2d((self.layer_3_num_zero_pad, 0, 0, 0)),\n",
    "                        Kerv1d(in_channels = 4*num_of_src_features,\n",
    "                               out_channels = 4*num_of_src_features,\n",
    "                               kernel_size = layer_3_kernel_size,\n",
    "                               stride = 1,\n",
    "                               padding = 0,\n",
    "                               dilation = layer_3_dilation_size,kernel_type = 'gaussian', \n",
    "                               learnable_kernel = True).to(device),\n",
    "                        nn.GELU()\n",
    "                        )       \n",
    "        layer_4_dilation_size = 8\n",
    "        layer_4_kernel_size = 2\n",
    "        self.layer_4_num_zero_pad = layer_4_dilation_size*(layer_4_kernel_size-1)\n",
    "        self.temp_conv_layer_4 = nn.Sequential(\n",
    "                        nn.ZeroPad2d((self.layer_4_num_zero_pad, 0, 0, 0)),\n",
    "                        Kerv1d(in_channels = 8*num_of_src_features,\n",
    "                               out_channels = 8*num_of_src_features,\n",
    "                               kernel_size = layer_4_kernel_size,\n",
    "                               stride = 1,\n",
    "                               padding = 0,\n",
    "                               dilation = layer_4_dilation_size,kernel_type = 'gaussian', \n",
    "                               learnable_kernel = True).to(device),\n",
    "                        nn.GELU()            \n",
    "                        )    \n",
    "    \n",
    "\n",
    "        \n",
    "        self.fully_connected = nn.Sequential(\n",
    "                        nn.Linear(16*num_of_src_features, dim_model)\n",
    "                        )   \n",
    "        \n",
    "    def forward(self, src): \n",
    "        \n",
    "        input_seq = src.permute(0,2,1) # out_size: batch, features, seq_len    \n",
    "        #print(input_seq.shape)\n",
    "        output_1 = self.temp_conv_layer_1(input_seq)  # out_size: batch, feature, seq_len\n",
    "           \n",
    "        input_seq = input_seq + output_1 # residual connection\n",
    "        input_seq = torch.cat((input_seq, output_1), dim=1) \n",
    "        #print(input_seq.shape,output_1.shape)\n",
    "        \n",
    "        output_2 = self.temp_conv_layer_2(input_seq)\n",
    "        \n",
    "        input_seq = input_seq + output_2 # residual connection\n",
    "        input_seq = torch.cat((input_seq, output_2), dim=1)\n",
    "        #print(input_seq.shape,output_2.shape)\n",
    "        \n",
    "        output_3 = self.temp_conv_layer_3(input_seq)\n",
    "        \n",
    "        input_seq = input_seq + output_3 # residual connection\n",
    "        input_seq = torch.cat((input_seq, output_3), dim=1)\n",
    "        #print(input_seq.shape,output_3.shape)\n",
    "        \n",
    "        output_4 = self.temp_conv_layer_4(input_seq)\n",
    "        \n",
    "        input_seq = input_seq + output_4 # residual connection\n",
    "        input_seq = torch.cat((input_seq, output_4), dim=1)\n",
    "        #print(input_seq.shape,output_4.shape)\n",
    "        \n",
    "        #output_5 = self.temp_conv_layer_5(input_seq)\n",
    "        \n",
    "        #input_seq = input_seq + output_5 # residual connection\n",
    "        #input_seq = torch.cat((input_seq, output_5), dim=1)\n",
    "        #print(input_seq.shape,output_5.shape)\n",
    "        \n",
    "        #input_seq = self.GeLU(input_seq)\n",
    "\n",
    "        output = input_seq.permute(0,2,1) # out_size: batch,  seq_len, feature\n",
    "        #print(input_seq.shape,output.shape) \n",
    "        #print('hello')\n",
    "        #output = torch.flatten(output, 1)\n",
    "\n",
    "        output = self.fully_connected(output)\n",
    "        #print(input_seq.shape,output.shape)\n",
    "        return output           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6b72fa",
   "metadata": {},
   "source": [
    "## Transformer_KCTNencoders with the TF based decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c326b6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SCTransformer_encoder_decoder(nn.Module):\n",
    "\n",
    "    # Constructor\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_model,\n",
    "        len_encoder,\n",
    "        dim_model_O_vehicle,\n",
    "        dim_model_T_vehicle,\n",
    "        num_heads,\n",
    "        num_O_vehicle_heads,\n",
    "        num_T_vehicle_heads,\n",
    "        num_encoder_layers,\n",
    "        num_encoder_layers_o_vehicle,\n",
    "        num_encoder_layers_T_vehicle,\n",
    "        num_decoder_layers,\n",
    "        dropout_p,\n",
    "        num_of_src_features,\n",
    "        num_of_tgt_features,\n",
    "        num_of_src_features_O_vehic=2,\n",
    "        num_of_src_features_T_vehic=5\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # INFO\n",
    "        self.model_type = \"SCTransformer_encoder_decoder\"\n",
    "        self.dim_model = dim_model\n",
    "        self.num_of_src_features = num_of_src_features\n",
    "        self.num_of_tgt_features = num_of_tgt_features\n",
    "        \n",
    "        #self.KCTNT = nn.Linear(  num_of_src_features_T_vehic,dim_model_T_vehicle)\n",
    "        self.KCTNT = KTCN_encoder( dim_model_T_vehicle, num_of_src_features_T_vehic)\n",
    "        self.KCTN1 = KTCN_encoder( dim_model_O_vehicle, num_of_src_features_O_vehic)\n",
    "        self.KCTN2 = KTCN_encoder( dim_model_O_vehicle, num_of_src_features_O_vehic)\n",
    "        self.KCTN3 = KTCN_encoder( dim_model_O_vehicle, num_of_src_features_O_vehic)\n",
    "        self.KCTN4 = KTCN_encoder( dim_model_O_vehicle, num_of_src_features_O_vehic)\n",
    "        self.KCTN5 = KTCN_encoder( dim_model_O_vehicle, num_of_src_features_O_vehic)\n",
    "        self.KCTN6 = KTCN_encoder( dim_model_O_vehicle, num_of_src_features_O_vehic)\n",
    "        \n",
    "        #self.multihead_anT = nn.MultiheadAttention(dim_model_T_vehicle, num_T_vehicle_heads,bias=True)\n",
    "        self.multihead_an1 = nn.MultiheadAttention(dim_model_O_vehicle, num_O_vehicle_heads,bias=True)\n",
    "        self.multihead_an2 = nn.MultiheadAttention(dim_model_O_vehicle, num_O_vehicle_heads,bias=True)\n",
    "        self.multihead_an3 = nn.MultiheadAttention(dim_model_O_vehicle, num_O_vehicle_heads,bias=True)\n",
    "        self.multihead_an4 = nn.MultiheadAttention(dim_model_O_vehicle, num_O_vehicle_heads,bias=True)\n",
    "        self.multihead_an5 = nn.MultiheadAttention(dim_model_O_vehicle, num_O_vehicle_heads,bias=True)\n",
    "        self.multihead_an6 = nn.MultiheadAttention(dim_model_O_vehicle, num_O_vehicle_heads,bias=True)\n",
    "        \n",
    "       \n",
    "        \n",
    "        #self.MHA_encoders_combine = nn.Linear(5+(2*dim_model_T_vehicle)+(6*dim_model_O_vehicle), dim_model)\n",
    "        #self.Linear1= nn.Linear(dim_model_T_vehicle+(6*dim_model_O_vehicle), dim_model)         \n",
    "        #self.Linear2= nn.Linear(dim_model_T_vehicle+(6*dim_model_O_vehicle), dim_model)      \n",
    "        self.Linear1= nn.Linear(6*dim_model_O_vehicle, dim_model)         \n",
    "        self.Linear2= nn.Linear(6*dim_model_O_vehicle, dim_model)    \n",
    "        self.Linear3= nn.Linear(dim_model_T_vehicle, dim_model)        \n",
    "        #self.embedding_encoder = nn.Linear(num_of_src_features, dim_model)\n",
    "\n",
    "        self.multihead_an = nn.MultiheadAttention(dim_model, num_heads,bias=True)      \n",
    "        #self.encoder_layer = nn.TransformerEncoderLayer(d_model=dim_model, nhead=num_heads, dropout=dropout_p)    \n",
    "        #self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_encoder_layers)  \n",
    "        self.Linear4= nn.Linear(2*dim_model, dim_model)  \n",
    "        \n",
    "\n",
    "        self.embedding_decoder = nn.Linear(2, dim_model)\n",
    "        self.decoder_layer = nn.TransformerDecoderLayer(d_model=dim_model, nhead=num_heads, dropout=dropout_p)    \n",
    "        self.transformer_decoder = nn.TransformerDecoder(self.decoder_layer, num_layers=num_decoder_layers)  \n",
    "        self.decoder3 = nn.Linear(dim_model,num_of_src_features_O_vehic)  \n",
    "    \n",
    "    def forward(self, src,tgt,tgt_mask=None, src_pad_mask=None, tgt_pad_mask=None): \n",
    "        #print(src.shape,tgt.shape)\n",
    "        #srcT=self.KCTNT(src[:,:,0:5])          \n",
    "        src1=self.KCTN1(src[:,:,[1,8]])\n",
    "        src2=self.KCTN2(src[:,:,[2,9]])\n",
    "        src3=self.KCTN3(src[:,:,[3,10]])\n",
    "        src4=self.KCTN4(src[:,:,[4,11]])\n",
    "        src5=self.KCTN5(src[:,:,[5,12]])\n",
    "        src6=self.KCTN6(src[:,:,[6,13]])        \n",
    "        srcT=self.KCTNT(src[:,:,[0,7,14,15,16]])  \n",
    "        ##attn_outputT, attn_output_weightsT = self.multihead_anT(srcT.permute(1,0,2),srcT.permute(1,0,2),srcT.permute(1,0,2))\n",
    "        \n",
    "        attn_output1, attn_output_weights1 = self.multihead_an1(srcT.permute(1,0,2),src1.permute(1,0,2),src1.permute(1,0,2))\n",
    "        attn_output2, attn_output_weights2 = self.multihead_an2(srcT.permute(1,0,2),src2.permute(1,0,2),src2.permute(1,0,2))\n",
    "        attn_output3, attn_output_weights3 = self.multihead_an3(srcT.permute(1,0,2),src3.permute(1,0,2),src3.permute(1,0,2))\n",
    "        attn_output4, attn_output_weights4 = self.multihead_an4(srcT.permute(1,0,2),src4.permute(1,0,2),src4.permute(1,0,2))\n",
    "        attn_output5, attn_output_weights5 = self.multihead_an5(srcT.permute(1,0,2),src5.permute(1,0,2),src5.permute(1,0,2))\n",
    "        attn_output6, attn_output_weights6 = self.multihead_an6(srcT.permute(1,0,2),src6.permute(1,0,2),src6.permute(1,0,2))                \n",
    "        \n",
    "        #Target_V_data=src[:,:,0:5]\n",
    "        attn_outputT = srcT.permute(1,0,2)\n",
    "        #print(attn_outputT.shape)\n",
    "        #src=torch.cat((Target_V_data,srcT,attn_outputT,attn_output1,attn_output2,attn_output3,attn_output4,attn_output5,attn_output6),dim=2)\n",
    "        src=torch.cat((attn_output1,attn_output2,attn_output3,attn_output4,attn_output5,attn_output6),dim=2)        \n",
    "        #src = src.permute(1,0,2)\n",
    "        #print(src.shape,attn_outputT.shape )\n",
    "        V,K=self.Linear1(src),self.Linear2(src)\n",
    "        Q=self.Linear3(attn_outputT)\n",
    "\n",
    "        #print(Q.shape,V.shape)\n",
    "        \n",
    "        attn_output, attn_output_weights= self.multihead_an(Q,K,V)  \n",
    "    \n",
    "        attn_output = torch.cat((attn_output,Q),dim=2).permute(1,0,2)\n",
    "        attn_output=self.Linear4(attn_output)\n",
    "        # Attention making code     \n",
    "        #print(attn_output.shape )\n",
    "\n",
    "\n",
    "            \n",
    "        tgt = self.embedding_decoder(tgt)  \n",
    "        tgt = tgt.permute(1,0,2)\n",
    "        transformer_out=self.transformer_decoder(tgt, attn_output.permute(1,0,2),tgt_mask)\n",
    "        transformer_out = transformer_out.permute(1,0,2)   \n",
    "        transformer_out = self.decoder3(transformer_out)\n",
    "\n",
    "        return transformer_out\n",
    "      \n",
    "    def get_tgt_mask(self, size) -> torch.tensor:\n",
    "        mask = torch.tril(torch.ones(size, size) == 1) # Lower triangular matrix\n",
    "        mask = mask.float()\n",
    "        mask = mask.masked_fill(mask == 0, float('-inf')) # Convert zeros to -inf\n",
    "        mask = mask.masked_fill(mask == 1, float(0.0)) # Convert ones to 0\n",
    "        \n",
    "        return mask\n",
    "    \n",
    "    def create_pad_mask(self, matrix: torch.tensor, pad_token: int) -> torch.tensor:\n",
    "        return (matrix == pad_token)       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "31bf42c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_model = 64\n",
    "num_heads = 8\n",
    "num_encoder_layers = 1\n",
    "num_decoder_layers = 1\n",
    "dropout_p = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "203a8db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_model_O_vehicle=64\n",
    "dim_model_T_vehicle=64\n",
    "\n",
    "num_O_vehicle_heads=8\n",
    "num_T_vehicle_heads=8\n",
    "\n",
    "num_encoder_layers_o_vehicle=1\n",
    "num_encoder_layers_T_vehicle=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e741b1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device=\"cpu\"\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bbb01dba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "99f3d1c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "246eafbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_model_2 = SCTransformer_encoder_decoder(dim_model = dim_model,\n",
    "                                           len_encoder=encoder_length,\n",
    "                                           dim_model_O_vehicle = dim_model_O_vehicle,\n",
    "                                           dim_model_T_vehicle = dim_model_T_vehicle,\n",
    "                                           num_heads = num_heads,\n",
    "                                           num_O_vehicle_heads=num_O_vehicle_heads,\n",
    "                                           num_T_vehicle_heads=num_T_vehicle_heads,\n",
    "                                           num_encoder_layers = num_encoder_layers,\n",
    "                                           num_encoder_layers_o_vehicle=num_encoder_layers_o_vehicle,\n",
    "                                           num_encoder_layers_T_vehicle=num_encoder_layers_T_vehicle,\n",
    "                                           num_decoder_layers = num_decoder_layers,\n",
    "                                           dropout_p = dropout_p,\n",
    "                                           num_of_src_features = num_of_src_features,\n",
    "                                           num_of_tgt_features = num_of_tgt_features,\n",
    "                                           num_of_src_features_O_vehic=2,\n",
    "                                           num_of_src_features_T_vehic=5).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5ba0566d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_Best_model2='Best_model_HSTA-KTCN-TF(-BP).pt'\n",
    "file_name_Best_model2_test='HSTA-KTCN-TF(-BP)_test_loss.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f1f2a3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.00005\n",
    "learning_rate=learning_rate*0.95\n",
    "opt = torch.optim.AdamW(tf_model_2.parameters(), lr = learning_rate)\n",
    "loss_fn = AllLoss()\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(opt, 5.0, gamma=0.8)\n",
    "teacher_forcing_ratio=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed97c2a-c818-4496-8a68-05e17a8fae0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for plotting later on    \n",
    "print(\"Training and validating model\")\n",
    "for epoch in range(0,100):\n",
    "    teacher_forcing_ratio=teacher_forcing_ratio-0.02\n",
    "    print(teacher_forcing_ratio)\n",
    "    \n",
    "    a = train_loop_new(epoch,tf_model_2,opt,loss_fn1,loss_fn,train_dataloader,teacher_forcing_ratio = teacher_forcing_ratio)   \n",
    "    final_validation_loss = final_validation_loop(tf_model_2,loss_fn1, loss_fn,val_dataloader,max_length)\n",
    "    print(final_validation_loss[-1,0])\n",
    "\n",
    "torch.save(tf_model_2.state_dict(),file_name_Best_model2) \n",
    "#pd.DataFrame(final_validation_loss).to_csv(file_name_Best_model2_test)                 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1a54e5-b4c4-4a04-92fc-303bca5f2a8c",
   "metadata": {},
   "source": [
    "## testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44d1fab-6ee5-42f4-8016-2aa8f39ed0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_model_2.load_state_dict(torch.load(file_name_Best_model2))\n",
    "learning_rate=0.00005\n",
    "learning_rate=learning_rate*0.95\n",
    "opt = torch.optim.AdamW(tf_model_2.parameters(), lr = learning_rate)\n",
    "loss_fn = AllLoss()\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(opt, 1.0, gamma=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239014fd-6381-4c83-a98f-e04f3ec7ae58",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_validation_loss = final_validation_loop(tf_model_2,loss_fn1, loss_fn,val_dataloader,max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e79b1a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(final_validation_loss).to_csv(file_name_Best_model2_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fa0188f2-e6a3-4214-b395-655944f0179c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HSTA-KTCN-TF(-BP)_test_loss.csv'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name_Best_model2_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47ab4a3-52b4-4857-944c-a13838e00e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE_loss=final_validation_loss[[4::5],0]\n",
    "RMSE_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0df2121-6dec-4e53-b288-37175cf7b5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAE_loss=final_validation_loss[[4::5],2]\n",
    "MAE_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
