{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fa80c9a",
   "metadata": {},
   "source": [
    "#  Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07fef787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version: 3.8.18 (default, Sep 11 2023, 13:39:12) [MSC v.1916 64 bit (AMD64)]\n",
      "Torch Version: 2.1.0\n",
      "Matplotlib Version: 3.5.3\n",
      "Numpy Version: 1.24.3\n",
      "sklearn Version: 1.1.3\n",
      "Pandas Version: 1.5.3\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "import time,os,sys,datetime\n",
    "\n",
    "import IPython\n",
    "import IPython.display\n",
    "\n",
    "#Plotting tools\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "#import seaborn as sns\n",
    "plt.style.use('ggplot')\n",
    "rcParams['figure.figsize'] = (14, 14)\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "%matplotlib inline\n",
    "import numpy\n",
    "#Computation tools\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "from six import iteritems\n",
    "import sklearn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import mat73\n",
    "import scipy.io as sio\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from pickle import dump\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader \n",
    "import torch.nn.functional as F\n",
    "#Printing out the Versions of libraries \n",
    "print('Python Version: {}'.format(sys.version))\n",
    "print('Torch Version: {}'.format(torch.__version__))\n",
    "print('Matplotlib Version: {}'.format(mpl.__version__))\n",
    "print('Numpy Version: {}'.format(np.__version__))\n",
    "print('sklearn Version: {}'.format(sklearn.__version__))\n",
    "print('Pandas Version: {}'.format(pd.__version__))\n",
    "#print('Seaborn Version: {}'.format(sns.__version__))\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import joblib\n",
    "#from sklearn.externals import joblib\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d25e9532",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_length = 15\n",
    "decoder_length=26\n",
    "input_length=encoder_length+decoder_length\n",
    "batch_size = 64\n",
    "stride_trining=decoder_length-1\n",
    "stride_testing=decoder_length-1\n",
    "copyies=1\n",
    "no_of_epoch = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780230ae",
   "metadata": {},
   "source": [
    "# Data loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7a077d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = mat73.loadmat(r'C:\\Users\\user\\Downloads\\New folder (2)\\review\\mypapers\\IJIT_KTCN\\R1\\Repos\\HIghD_test_data.mat')\n",
    "#data_x_train = data_dict['train_data']\n",
    "data_x_test = data_dict['Test_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a63298e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12484, 1916)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bd2d68",
   "metadata": {},
   "source": [
    "# Actual Trajectory Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8127767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 600.0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x1400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_features = data_x_test[1900][0][:,0:2]\n",
    "plot_features =pd.DataFrame(plot_features)\n",
    "#plot_features = dataset_fridge['fridge'][155000 : 156500]\n",
    "#_ = plot_features.plot(subplots=False)\n",
    "plt.plot(plot_features[0],plot_features[1])\n",
    "plt.xlim(0, 60)\n",
    "plt.ylim(0, 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f2dcca",
   "metadata": {},
   "source": [
    "## Timeseries making"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4277c490",
   "metadata": {},
   "outputs": [],
   "source": [
    "class timeseries(Dataset):\n",
    "    def __init__(self,x,y,z):\n",
    "        self.x = torch.tensor(x,dtype=torch.float32)\n",
    "        self.y = torch.tensor(y,dtype=torch.float32)\n",
    "        self.z = torch.tensor(z,dtype=torch.float32)        \n",
    "        self.len = x.shape[0]\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return self.x[idx],self.y[idx],self.z[idx]\n",
    "  \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "#train_x_data_tensor_3d=train_x_data_tensor_3d.numpy()\n",
    "#train_x_data_tensor_3d=train_x_data_tensor_3d.repeat(1,1,copyies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dcbbc0",
   "metadata": {},
   "source": [
    "### Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0d4067b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AllLoss(nn.Module):\n",
    "    def __init__(self, eps=1e-7):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        \n",
    "        self.mse = nn.MSELoss()\n",
    "        self.mae=nn.L1Loss()\n",
    "        self.KLD=nn.KLDivLoss(reduction=\"batchmean\")\n",
    "        \n",
    "    def rmse(self,a,b):\n",
    "        return torch.sqrt(self.mse(a,b) + self.eps)  \n",
    "\n",
    "        \n",
    "    def forward(self,yhat,y):\n",
    "        #RMSE_loss = torch.sqrt(self.mse(yhat,y) + self.eps)\n",
    "        RMSE_loss = self.rmse(yhat,y)\n",
    "        MSE_Loss= self.mse(yhat,y)\n",
    "        MAE_Loss=self.mae(yhat,y)\n",
    "        #KLD_Loss_softmax=self.KLD(F.softmax(yhat),F.softmax(y))\n",
    "        L1=torch.sqrt(self.mse(yhat[:,:,0],y[:,:,0]) + self.eps)\n",
    "        L2=torch.sqrt(self.mse(yhat[:,:,1],y[:,:,1]) + self.eps) \n",
    "        KLD_Loss=self.KLD(yhat,y)\n",
    "        return [1.41*RMSE_loss,2*MSE_Loss,2*MAE_Loss,L1,L2,KLD_Loss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3131fff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn1=nn.CrossEntropyLoss()\n",
    "loss_fn = AllLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424bf392",
   "metadata": {},
   "source": [
    "### Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea467bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim_model, dropout_p, max_len):\n",
    "        super().__init__()\n",
    "        # Modified version from: https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "        # max_len determines how far the position can have an effect on a token (window)\n",
    "        \n",
    "        # Info\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "        # Encoding - From formula\n",
    "        pos_encoding = torch.zeros(max_len, dim_model)\n",
    "        positions_list = torch.arange(0, max_len, dtype=torch.float).view(-1, 1) # 0, 1, 2, 3, 4, 5\n",
    "        division_term = torch.exp(torch.arange(0, dim_model, 2).float() * (-math.log(10000.0)) / dim_model) # 1000^(2i/dim_model)\n",
    "        \n",
    "        # PE(pos, 2i) = sin(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 0::2] = torch.sin(positions_list * division_term)\n",
    "        \n",
    "        # PE(pos, 2i + 1) = cos(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 1::2] = torch.cos(positions_list * division_term)\n",
    "        \n",
    "        # Saving buffer (same as parameter without gradients needed)\n",
    "        pos_encoding = pos_encoding.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer(\"pos_encoding\",pos_encoding)\n",
    "        \n",
    "    def forward(self, token_embedding: torch.tensor) -> torch.tensor:\n",
    "        # Residual connection + pos encoding\n",
    "        return self.dropout(token_embedding + self.pos_encoding[:token_embedding.size(0), :])\n",
    "        #return  self.pos_encoding[:token_embedding.size(0), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f4a0c7",
   "metadata": {},
   "source": [
    "# Model for reframing data by host vehicle taking 6 sourrounding vehicle and different mixed teacher training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3858a22a",
   "metadata": {},
   "source": [
    "##  with out normalization of data x,y cordinate changed (host vehicle cordinate are frame of reference)and 3D tensor making train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5b7df37",
   "metadata": {},
   "outputs": [],
   "source": [
    "stride_trining_new=5\n",
    "stride_testing_new=5\n",
    "avg_l_x=10\n",
    "avg_l_v=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "def3c623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are many features in the HighD `dataset, here, only positional features have been taken \n",
    "training_features=[2,10,18,26,34,42,50,3,11,19,27,35,43,51,4,5,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c59e61f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_features=  [0,1]\n",
    "testing_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1be97bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Raw_x_training_data= []      \n",
    "Raw_y_training_data= []  \n",
    "Raw_z_training_data= []  \n",
    "for i in range(0,len(data_x_train)):\n",
    "    x = torch.tensor(data_x_train[i][0][:,training_features]).clone().detach()\n",
    "    y = torch.tensor(data_x_train[i][0][:,testing_features]).clone().detach()    \n",
    "    #x= x[::2].clone().detach()\n",
    "    #y= y[::2].clone().detach()\n",
    "    x= x[::5].clone().detach()\n",
    "    y= y[::5].clone().detach()    \n",
    "    if len(x)>input_length:\n",
    "        for j in range(0,len(x)+1-encoder_length-decoder_length-max(avg_l_x,avg_l_v),stride_trining_new):\n",
    "            a=x[j:j+encoder_length][:].clone().detach()\n",
    "            b=y[j+encoder_length:j+encoder_length+decoder_length][:].clone().detach()\n",
    "            d_new= torch.zeros(b.shape[0], 2)\n",
    "            for k in range (0, b.shape[0]):\n",
    "                d=[]\n",
    "                if torch.mean(x[j+encoder_length+k : j+encoder_length+k+avg_l_x,14])<-0.2:\n",
    "                    if 0.8*x[j+encoder_length+k-1,15]> torch.mean(x[j+encoder_length+k:j+encoder_length+k+avg_l_v,15]):\n",
    "                        d=[0,0]\n",
    "                    elif 1.2*x[j+encoder_length+k-1,15]< torch.mean(x[j+encoder_length+k:j+encoder_length+k+avg_l_v,15]):\n",
    "                        d=[0,2]\n",
    "                    else:\n",
    "                        d=[0,1]\n",
    "                            \n",
    "                elif torch.mean(x[j+encoder_length+k : j+encoder_length+k+avg_l_x,14])>0.2:\n",
    "                    \n",
    "                    if 0.8*x[j+encoder_length+k-1,15]> torch.mean(x[j+encoder_length+k : j+encoder_length+k+avg_l_v,15]):\n",
    "                        d=[2,0]\n",
    "                    elif 1.2*x[j+encoder_length+k-1,15]< torch.mean(x[j+encoder_length+k:j+encoder_length+k+avg_l_v,15]):\n",
    "                        d=[2,2]\n",
    "                    else:\n",
    "                        d=[2,1]      \n",
    "                else:\n",
    "                    if 0.8*x[j+encoder_length+k-1,15]> torch.mean(x[j+encoder_length+k:j+encoder_length+k+avg_l_v,15]):\n",
    "                        d=[1,0]\n",
    "                    elif 1.2*x[j+encoder_length+k-1,15]< torch.mean(x[j+encoder_length+k:j+encoder_length+k+avg_l_v,15]):\n",
    "                        d=[1,2]\n",
    "                    else:\n",
    "                        d=[1,1]    \n",
    "                #print(d)\n",
    "                d_new[k,:]=torch.tensor(d).clone().detach()\n",
    "            del d\n",
    "            c=a[-1,[0,7]].clone().detach()\n",
    "            \n",
    "            #a[:,0:2]=a[:,0:2]-c   \n",
    "            a[:,0:7]=a[:,0:7]-c[0] \n",
    "            a[:,7:14]=a[:,7:14]-c[1]  \n",
    "            b[:,0:2]=b[:,0:2]-c\n",
    "            \n",
    "            Raw_x_training_data.append(a.clone().detach())\n",
    "            Raw_y_training_data.append(b.clone().detach())\n",
    "            Raw_z_training_data.append(d_new.clone().detach())\n",
    "            del a,b,c,d_new\n",
    "        #print(d_new)    \n",
    "    del x,y\n",
    "    \n",
    "Raw_x_training_data = torch.stack(Raw_x_training_data, 0)            \n",
    "Raw_y_training_data = torch.stack(Raw_y_training_data, 0) \n",
    "Raw_z_training_data = torch.stack(Raw_z_training_data, 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07a54a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "Raw_x_testing_data= []      \n",
    "Raw_y_testing_data= []  \n",
    "Raw_z_testing_data= []  \n",
    "for i in range(0,len(data_x_test)):\n",
    "#for i in range(0,20):\n",
    "    #x = torch.tensor(data_x_test[i][0][:,0:training_features]).clone().detach()\n",
    "    #y = torch.tensor(data_x_test[i][0][:,0:testing_features]).clone().detach()\n",
    "    x = torch.tensor(data_x_test[i][0][:,training_features]).clone().detach()\n",
    "    y = torch.tensor(data_x_test[i][0][:,testing_features]).clone().detach()\n",
    "    \n",
    "    #x= x[::2].clone().detach()\n",
    "    #y= y[::2].clone().detach()\n",
    "    x= x[::5].clone().detach()\n",
    "    y= y[::5].clone().detach()   \n",
    "    if len(x)>input_length:\n",
    "        for j in range(0,len(x)+1-encoder_length-decoder_length-max(avg_l_x,avg_l_v),stride_trining_new):\n",
    "            a=x[j:j+encoder_length][:].clone().detach()\n",
    "            b=y[j+encoder_length:j+encoder_length+decoder_length][:].clone().detach()\n",
    "            d_new= torch.zeros(b.shape[0], 2)\n",
    "            for k in range (0, b.shape[0]):\n",
    "                d=[]\n",
    "                if torch.mean(x[j+encoder_length+k : j+encoder_length+k+avg_l_x,14])<-0.2:\n",
    "                    if 0.8*x[j+encoder_length+k-1,15]> torch.mean(x[j+encoder_length+k:j+encoder_length+k+avg_l_v,15]):\n",
    "                        d=[0,0]\n",
    "                    elif 1.2*x[j+encoder_length+k-1,15]< torch.mean(x[j+encoder_length+k:j+encoder_length+k+avg_l_v,15]):\n",
    "                        d=[0,2]\n",
    "                    else:\n",
    "                        d=[0,1]\n",
    "                            \n",
    "                elif torch.mean(x[j+encoder_length+k : j+encoder_length+k+avg_l_x,14])>0.2:\n",
    "                    \n",
    "                    if 0.8*x[j+encoder_length+k-1,15]> torch.mean(x[j+encoder_length+k : j+encoder_length+k+avg_l_v,15]):\n",
    "                        d=[2,0]\n",
    "                    elif 1.2*x[j+encoder_length+k-1,15]< torch.mean(x[j+encoder_length+k:j+encoder_length+k+avg_l_v,15]):\n",
    "                        d=[2,2]\n",
    "                    else:\n",
    "                        d=[2,1]      \n",
    "                else:\n",
    "                    if 0.8*x[j+encoder_length+k-1,15]> torch.mean(x[j+encoder_length+k:j+encoder_length+k+avg_l_v,15]):\n",
    "                        d=[1,0]\n",
    "                    elif 1.2*x[j+encoder_length+k-1,15]< torch.mean(x[j+encoder_length+k:j+encoder_length+k+avg_l_v,15]):\n",
    "                        d=[1,2]\n",
    "                    else:\n",
    "                        d=[1,1]    \n",
    "                #print(d)\n",
    "                d_new[k,:]=torch.tensor(d).clone().detach()\n",
    "            del d\n",
    "            \n",
    "            c=a[-1,[0,7]].clone().detach()\n",
    "            #a[:,0:7]=a[:,0:7]-c       \n",
    "            a[:,0:7]=a[:,0:7]-c[0] \n",
    "            a[:,7:14]=a[:,7:14]-c[1]  \n",
    "            b[:,0:2]=b[:,0:2]-c            \n",
    "            \n",
    "            Raw_x_testing_data.append(a.clone().detach())            \n",
    "            Raw_y_testing_data.append(b.clone().detach())\n",
    "            Raw_z_testing_data.append(d_new.clone().detach())\n",
    "            del a,b,c,d_new\n",
    "    del x,y\n",
    "    \n",
    "Raw_x_testing_data = torch.stack(Raw_x_testing_data, 0)            \n",
    "Raw_y_testing_data = torch.stack(Raw_y_testing_data, 0)\n",
    "Raw_z_testing_data = torch.stack(Raw_z_testing_data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865b28cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Raw_x_training_data.shape,Raw_y_training_data.shape,Raw_z_training_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6267d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Raw_x_testing_data.shape,Raw_y_testing_data.shape,Raw_z_testing_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eda19ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "del data_dict,data_x_train ,data_x_test "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4f4b09",
   "metadata": {},
   "source": [
    "##   training and testing  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a85a5c",
   "metadata": {},
   "source": [
    "## Teacher forcing inside sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d606caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop_new(epoch,model, opt,loss_fn1,loss_fn,dataloader,teacher_forcing_ratio):\n",
    "    model.train()\n",
    "    total_loss=0.\n",
    "    cur_loss=0.\n",
    "    cur_loss1=0.\n",
    "    cur_loss2=0.\n",
    "    cur_loss3=0.\n",
    "    for batch,data in enumerate(dataloader):      \n",
    "        current_lr = scheduler.get_lr()[0]\n",
    "        X=data[0].clone().detach()\n",
    "        Y=data[1].clone().detach()\n",
    "        Z=data[2].clone().detach()\n",
    "        #print(X.shape,Y.shape,Z.shape)\n",
    "        Encoder_input=X.clone().detach().to(device) \n",
    "        Decoder_input=Y[:,:-1,:].clone().detach().to(device) \n",
    "        Decoder_output=Y[:,1:,:].clone().detach().to(device) \n",
    "        Encoder_input=torch.tensor(Encoder_input, dtype=torch.float32).to(device)   \n",
    "        Encoder_output=Z[:,:,:].clone().detach().to(device)\n",
    "        Encoder_output = torch.tensor(Encoder_output,dtype=torch.long)\n",
    "        \n",
    "        #if random.random() < teacher_forcing_ratio:   \n",
    "        if teacher_forcing_ratio==1.0:\n",
    "            sequence_length = Decoder_input.shape[1]\n",
    "            tgt_mask = model.get_tgt_mask(sequence_length).to(device) \n",
    "            Decoder_input=Y[:,:-1,:].clone().detach().to(device) \n",
    "            #Decoder_input=torch.cat((Decoder_input,Z[:,:-1,:].clone().detach().to(device) ), dim=2)\n",
    "            Decoder_input=torch.tensor(Decoder_input, dtype=torch.float32).to(device)\n",
    "            tgt_mask=torch.tensor(tgt_mask, dtype=torch.float32).to(device)  \n",
    "            #opt.zero_grad()  \n",
    "        \n",
    "            a1,a2,pred = model(Encoder_input,Decoder_input, tgt_mask)\n",
    "\n",
    "        \n",
    "            \n",
    "            \n",
    "            \n",
    "            del sequence_length,tgt_mask,Decoder_input\n",
    "        else:\n",
    "            del Decoder_input\n",
    "            opt.zero_grad()\n",
    "            SOS_token = Y[:,0,:].clone().detach().to(device) \n",
    "            #SOS_token2 = Z[:,0,:].clone().detach().to(device) \n",
    "            Decoder_input = SOS_token.unsqueeze(1).to(device)  \n",
    "            #Decoder_input=torch.cat((Decoder_input,SOS_token2.unsqueeze(1).to(device)), dim=2)\n",
    "            max_length=Decoder_output.shape[1]\n",
    "            #loss_real = torch.empty(size=(max_length, 6))            \n",
    "            #print(batch,'gello')\n",
    "            sequence_length = Decoder_input.shape[1]\n",
    "            tgt_mask = model.get_tgt_mask(sequence_length).to(device)  \n",
    "            Decoder_input=torch.tensor(Decoder_input, dtype=torch.float32).to(device)\n",
    "            tgt_mask=torch.tensor(tgt_mask, dtype=torch.float32).to(device)\n",
    "            a1,a2,pred1 = model(Encoder_input,Decoder_input, tgt_mask)\n",
    "            \n",
    "            ##loss3 = loss_fn.rmse(pred1, Decoder_output[:,0:pred1.shape[1],:])\n",
    "            \n",
    "                     \n",
    "             \n",
    "            Encoder_output_for_loss=Encoder_output[0:a1.shape[0],0,:] \n",
    "            #dtype=torch.long\n",
    "            #print(a1.shape,a2.shape,Encoder_output[:,0].shape)\n",
    "            ##loss1 = loss_fn1(a1,Encoder_output_for_loss[:,0])\n",
    "            ##loss2 = loss_fn1(a2,Encoder_output_for_loss[:,1])\n",
    "            #print(loss1.item(),loss2.item(),loss3.item())\n",
    "            ##loss=loss1+loss2+loss3\n",
    "                                   \n",
    "            ##loss.backward()\n",
    "            ##torch.nn.utils.clip_grad_norm_(model.parameters(), 0.7)\n",
    "            ##opt.step()   \n",
    "            ##opt.zero_grad() \n",
    "            #print(pred1.shape,pred1.shape[1])\n",
    "            del sequence_length,tgt_mask#,Decoder_input#,loss\n",
    "\n",
    "            for i in range(1,max_length-1):\n",
    "                #print(i)\n",
    "                SOS_token = Y[:,0,:].clone().detach().to(device)\n",
    "                \n",
    "                if random.random() < teacher_forcing_ratio:   \n",
    "                    Decoder_input = torch.cat(  (Decoder_input, Y[:,i,:].unsqueeze(1).to(device) ), dim=1)\n",
    "                else:\n",
    "                    Decoder_input = torch.cat(   (Decoder_input, pred1[:,-1,:].unsqueeze(1)), dim=1)\n",
    "                \n",
    "                \n",
    "                #Decoder_input = torch.cat((Decoder_input,Z[:,0:i+1,:].clone().detach().to(device)), dim=2)\n",
    "                sequence_length = Decoder_input.shape[1]\n",
    "                tgt_mask = model.get_tgt_mask(sequence_length).to(device)  \n",
    "                Decoder_input=torch.tensor(Decoder_input, dtype=torch.float32).to(device)\n",
    "                tgt_mask=torch.tensor(tgt_mask, dtype=torch.float32).to(device)\n",
    "                #print(Encoder_input.shape,Decoder_input.shape, tgt_mask.shape)\n",
    "                a1,a2,pred1 = model(Encoder_input,Decoder_input, tgt_mask) \n",
    "                \n",
    "                ##loss3 = loss_fn.rmse(pred1, Decoder_output[:,0:pred1.shape[1],:])\n",
    "            \n",
    "                Encoder_output_for_loss=Encoder_output[0:a1.shape[0],0,:]          \n",
    "  \n",
    "                opt.zero_grad() \n",
    "            \n",
    "            \n",
    "                del sequence_length,tgt_mask,a1,a2,Encoder_output_for_loss#,loss,loss1,loss2,loss3\n",
    "            \n",
    "            \n",
    "            if random.random() < teacher_forcing_ratio:   \n",
    "                Decoder_input = torch.cat(  (Decoder_input, Y[:,max_length-1,:].unsqueeze(1).to(device) ), dim=1)\n",
    "            else:\n",
    "                Decoder_input = torch.cat(   (Decoder_input, pred1[:,-1,:].unsqueeze(1)), dim=1)\n",
    "                     \n",
    "            \n",
    "            \n",
    "            \n",
    "           ## Decoder_input = torch.cat((SOS_token.unsqueeze(1), pred1), dim=1)    \n",
    "            #Decoder_input = torch.cat((Decoder_input,Z[:,0:-1,:].clone().detach().to(device)), dim=2)\n",
    "                \n",
    "            sequence_length = Decoder_input.shape[1]\n",
    "            tgt_mask = model.get_tgt_mask(sequence_length).to(device)  \n",
    "            Decoder_input=torch.tensor(Decoder_input, dtype=torch.float32).to(device)\n",
    "            tgt_mask=torch.tensor(tgt_mask, dtype=torch.float32).to(device)        \n",
    "               \n",
    "            a1,a2,pred = model(Encoder_input,Decoder_input, tgt_mask)\n",
    "            #print(Encoder_input.shape,Decoder_input.shape)\n",
    "            del SOS_token,sequence_length,tgt_mask,Decoder_input   \n",
    "        #print(pred.shape, Decoder_output.shape)    \n",
    "        loss3 = loss_fn.rmse(pred, Decoder_output[:,0:pred.shape[1],:])\n",
    "        \n",
    "        Encoder_output_for_loss=Encoder_output[0:a1.shape[0],0,:]          \n",
    "        #Encoder_output = torch.tensor(Encoder_output,dtype=torch.long) \n",
    "        dtype=torch.long\n",
    "        #print(a1.shape,a2.shape,Encoder_output[:,0].shape)\n",
    "        loss1 = loss_fn1(a1,Encoder_output_for_loss[:,0])\n",
    "        loss2 = loss_fn1(a2,Encoder_output_for_loss[:,1])\n",
    "        #print(loss1.item(),loss2.item(),loss3.item())\n",
    "        loss=loss1+loss2+loss3\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #print(loss.shape)\n",
    "        #opt.zero_grad()\n",
    "        loss.backward()\n",
    "        #loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.7)\n",
    "        opt.step()   \n",
    "        opt.zero_grad()\n",
    "        cur_loss += loss.detach().item()\n",
    "        cur_loss1 += loss1.detach().item()\n",
    "        cur_loss2 += loss2.detach().item()\n",
    "        cur_loss3 += loss3.detach().item()\n",
    "        \n",
    "        total_loss += loss.detach().item()\n",
    "        \n",
    "        \n",
    "        if batch % 500 == 0 and batch > 0:\n",
    "            print('tr','|epoch {:3d}|'.format(epoch+1),\n",
    "                  '|lr {:02.6f}|'.format(scheduler.get_lr()[0]),\n",
    "                  '|{:5d}/{:5d} batches|'.format(batch,len(dataloader)),\n",
    "                  'loss {:8.6f}|'.format(loss),\n",
    "                  'loss per 500 batch {:8.3f}| {:8.3f} |{:8.3f}|{:8.3f}|'.format(cur_loss,cur_loss1,cur_loss2,cur_loss3))        \n",
    "            cur_loss = 0.\n",
    "            cur_loss1 = 0.\n",
    "            cur_loss2 = 0.\n",
    "            cur_loss3 = 0.\n",
    "        del pred,loss,X,Y,Encoder_input,Decoder_output,loss1,loss2,loss3,a1,a2,Encoder_output_for_loss\n",
    "                \n",
    "    scheduler.step()   \n",
    "    return total_loss / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c5fb1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_validation_loop(model,loss_fn1, loss_fn,dataloader,max_length):\n",
    "    #model.eval()\n",
    "    \n",
    "    ##loss_real_list=[]\n",
    "    loss_avg_real= torch.zeros(max_length, 8)\n",
    "    #print(loss_avg_real)\n",
    "    with torch.no_grad():\n",
    "        for batch,data in enumerate(dataloader):     \n",
    "            #print(batch)\n",
    "            X=data[0].clone().detach()\n",
    "            Y=data[1].clone().detach()\n",
    "            Z=data[2].clone().detach()\n",
    "            #print(X.shape,Y.shape,Z.shape)\n",
    "            Encoder_input=X.clone().detach().to(device) \n",
    "            #Decoder_input=Y[:,:-1,:].clone().detach().to(device) \n",
    "            Decoder_output=Y[:,1:,:].clone().detach().to(device) \n",
    "            Encoder_input=torch.tensor(Encoder_input, dtype=torch.float32).to(device)   \n",
    "            Encoder_output=Z[:,:,:].clone().detach().to(device)\n",
    "            Encoder_output = torch.tensor(Encoder_output,dtype=torch.long)\n",
    "        \n",
    "            SOS_token = Y[:,0,:].clone().detach().to(device) \n",
    "            Decoder_input = SOS_token.unsqueeze(1).to(device)  \n",
    "            max_length=Decoder_output.shape[1]\n",
    "            loss_real = torch.zeros(max_length, 8)\n",
    "            \n",
    "            sequence_length = Decoder_input.shape[1]\n",
    "            tgt_mask = model.get_tgt_mask(sequence_length).to(device)  \n",
    "            Decoder_input=torch.tensor(Decoder_input, dtype=torch.float32).to(device)\n",
    "            tgt_mask=torch.tensor(tgt_mask, dtype=torch.float32).to(device)\n",
    "            \n",
    "            a1,a2,pred1 = model(Encoder_input,Decoder_input, tgt_mask)\n",
    "            \n",
    "            loss3 = loss_fn.rmse(pred1, Decoder_output[:,0:pred1.shape[1],:])\n",
    "            Encoder_output_for_loss=Encoder_output[0:a1.shape[0],0,:] \n",
    "            loss1 = loss_fn1(a1,Encoder_output_for_loss[:,0])\n",
    "            loss2 = loss_fn1(a2,Encoder_output_for_loss[:,1])\n",
    "            \n",
    "            loss = loss_fn(pred1, Decoder_output[:,0:pred1.shape[1],:])  \n",
    "                \n",
    "            loss = torch.Tensor(loss) \n",
    "            #print(loss.shape[1])\n",
    "            loss_real[0,0:6] = loss.clone().detach()       \n",
    "\n",
    "            del sequence_length,tgt_mask,Decoder_input,loss,loss1,loss2,loss3,a1,a2,Encoder_output_for_loss    \n",
    "            #print(batch,'gello')\n",
    "            for i in range(1,max_length):\n",
    "                Decoder_input = torch.cat((SOS_token.unsqueeze(1), pred1), dim=1)\n",
    "                sequence_length = Decoder_input.shape[1]\n",
    "                tgt_mask = model.get_tgt_mask(sequence_length).to(device)  \n",
    "                \n",
    "                Decoder_input=torch.tensor(Decoder_input, dtype=torch.float32).to(device)\n",
    "                tgt_mask=torch.tensor(tgt_mask, dtype=torch.float32).to(device)\n",
    "                \n",
    "                a1,a2,pred1= model(Encoder_input,Decoder_input, tgt_mask)      \n",
    "                            \n",
    "                loss3 = loss_fn.rmse(pred1, Decoder_output[:,0:pred1.shape[1],:])\n",
    "                Encoder_output_for_loss=Encoder_output[0:a1.shape[0],0,:] \n",
    "                loss1 = loss_fn1(a1,Encoder_output_for_loss[:,0])\n",
    "                loss2 = loss_fn1(a2,Encoder_output_for_loss[:,1])\n",
    "            \n",
    "                loss = loss_fn(pred1, Decoder_output[:,0:pred1.shape[1],:])  \n",
    "                \n",
    "                loss = torch.Tensor(loss) \n",
    "                loss_real[i,0:6] = loss.clone().detach()\n",
    "\n",
    "                #loss_real[i] = loss.clone().detach()     \n",
    "                #print(loss_real.shape,loss.shape)\n",
    "                #print('hello')\n",
    "                #print(loss_real[i],loss,loss_real)\n",
    "                #print('sello')\n",
    "                del Decoder_input,loss,loss1,loss2,loss3,a1,a2,Encoder_output_for_loss                       \n",
    "\n",
    "                \n",
    "            ##loss_real_list+= [loss_real.clone().detach()]\n",
    "            loss_avg_real+= loss_real.clone().detach()\n",
    "            \n",
    "            #print(loss_real.shape,loss_avg_real.shape,len(loss_real_list))\n",
    "            del loss_real,pred1   \n",
    "            #print(loss_avg_real)\n",
    "            #print('tello')\n",
    "    return loss_avg_real/ len(dataloader)#,loss_real_list\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77181452",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_src_features = Raw_x_testing_data.shape[2]\n",
    "num_of_tgt_features = Raw_y_testing_data.shape[2]\n",
    "num_of_tgt_features2= Raw_z_testing_data.shape[2]\n",
    "#del all_dataloader,val_dataloader\n",
    "\n",
    "t_dataset = timeseries(Raw_x_training_data.repeat(1,1,copyies).numpy(),Raw_y_training_datan.numpy(),Raw_z_training_datan.numpy())\n",
    "v_dataset = timeseries(Raw_x_testing_data.repeat(1,1,copyies).numpy(),Raw_y_testing_data.numpy(),Raw_z_testing_data.numpy())\n",
    "\n",
    "train_dataloader = DataLoader(t_dataset,shuffle=True,batch_size=batch_size)   \n",
    "val_dataloader = DataLoader(v_dataset,shuffle=False,batch_size=batch_size) \n",
    "\n",
    "#del Raw_x_training_data,Raw_y_training_data,Raw_z_training_data,Z_train,t_dataset\n",
    "#del Raw_x_testing_data,Raw_y_testing_data,Raw_z_testing_data,Z_test,v_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e3aa09e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length=Raw_y_testing_data.shape[1] -1\n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "90af3bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "del t_dataset,v_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "103600e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "del  Raw_x_training_data,Raw_y_training_data,Raw_z_training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fccaaad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "del Raw_x_testing_data, Raw_y_testing_data, Raw_z_testing_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b781545e",
   "metadata": {},
   "source": [
    "## Model HSTA-KCTN-TF(-BP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05c28a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Kerv1d(nn.Conv1d):\n",
    "    r\"\"\"Applies a 1D kervolution over an input signal composed of several input\n",
    "        planes.\n",
    "        Args:\n",
    "            in_channels (int): Number of channels in the input image\n",
    "            out_channels (int): Number of channels produced by the convolution\n",
    "            kernel_size (int or tuple): Size of the convolving kernel\n",
    "            stride (int or tuple, optional): Stride of the convolution. Default: 1\n",
    "            padding (int or tuple, optional): Zero-padding added to both sides of\n",
    "                the input. Default: 0\n",
    "            padding_mode (string, optional). Accepted values `zeros` and `circular` Default: `zeros`\n",
    "            dilation (int or tuple, optional): Spacing between kernel\n",
    "                elements. Default: 1\n",
    "            groups (int, optional): Number of blocked connections from input\n",
    "                channels to output channels. Default: 1\n",
    "            bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``\n",
    "            kernel_type (str), Default: 'linear'\n",
    "            learnable_kernel (bool): Learnable kernel parameters.  Default: False \n",
    "            balance=1, power=3, gamma=1\n",
    "        Shape:\n",
    "            - Input: :math:`(N, C_{in}, L_{in})`\n",
    "            - Output: :math:`(N, C_{out}, L_{out})` where\n",
    "            .. math::\n",
    "                L_{out} = \\left\\lfloor\\frac{L_{in} + 2 \\times \\text{padding} - \\text{dilation}\n",
    "                            \\times (\\text{kernel\\_size} - 1) - 1}{\\text{stride}} + 1\\right\\rfloor\n",
    "        Examples::\n",
    "            >>> m = Kerv1d(16, 33, 3, kernel_type='polynomial', learnable_kernel=True)\n",
    "            >>> input = torch.randn(20, 16, 50)\n",
    "            >>> output = m(input)\n",
    "        .. _kervolution:\n",
    "            https://arxiv.org/pdf/1904.03955.pdf\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, \n",
    "            stride=1, padding=0, dilation=2, groups=1, bias=True, padding_mode='zeros',\n",
    "            kernel_type='linear', learnable_kernel=False, balance=1, power=3, gamma=1):\n",
    "\n",
    "        super(Kerv1d, self).__init__(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias, padding_mode)\n",
    "        self.kernel_type, self.learnable_kernel = kernel_type, learnable_kernel\n",
    "        self.balance, self.power, self.gamma = balance, power, gamma\n",
    "        self.unfold = nn.Unfold((kernel_size,1), (dilation,1), (padding, 0), (stride,1))\n",
    "\n",
    "        # parameter for kernels\n",
    "        if learnable_kernel == True:\n",
    "            self.balance = nn.Parameter(torch.FloatTensor([balance] * out_channels)).view(-1, 1).to(device)\n",
    "            self.gamma   = nn.Parameter(torch.FloatTensor([gamma]   * out_channels)).view(-1, 1).to(device)\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = self.unfold(input.unsqueeze(-1)).unsqueeze(1)\n",
    "        weight  = self.weight.view(self.out_channels, -1, 1)\n",
    "\n",
    "        if self.kernel_type == 'linear':\n",
    "            output = (input * weight).sum(dim=2)\n",
    "\n",
    "        elif self.kernel_type == 'manhattan':\n",
    "            output = -((input - weight).abs().sum(dim=2))\n",
    "\n",
    "        elif self.kernel_type == 'euclidean':\n",
    "            output = -(((input - weight)**2).sum(dim=2))\n",
    "\n",
    "        elif self.kernel_type == 'polynomial':\n",
    "            output = ((input * weight).sum(dim=2) + self.balance)**self.power\n",
    "\n",
    "        elif self.kernel_type == 'gaussian':\n",
    "            output = (-self.gamma*((input - weight)**2).sum(dim=2)).exp() + 0\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError(self.kernel_type+' Kerv1d not implemented')\n",
    "\n",
    "        if self.bias is not None:\n",
    "            output += self.bias.view(self.out_channels, -1)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "    def cuda(self, device=None):\n",
    "        if self.learnable_kernel == True:\n",
    "            self.balance = self.balance.cuda(device)\n",
    "            self.gamma = self.gamma.cuda(device)\n",
    "        return self._apply(lambda t: t.cuda(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef5b624",
   "metadata": {},
   "source": [
    "## KTCN based encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ffd4a899",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KTCN_encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, dim_model, num_of_src_features):\n",
    "        \n",
    "        super(KTCN_encoder, self).__init__()\n",
    "        \n",
    "        #self.dim_model = dim_model\n",
    "        self.num_of_src_features = num_of_src_features\n",
    "        self.num_of_tgt_features = num_of_tgt_features\n",
    "\n",
    "        self.GeLU = nn.GELU()    \n",
    "            \n",
    "        layer_1_dilation_size = 1\n",
    "        layer_1_kernel_size = 2\n",
    "        #self.layer_1_dilation_size = layer_1_dilation_size\n",
    "        #self.layer_1_kernel_size = layer_1_kernel_size\n",
    "        \n",
    "        self.layer_1_num_zero_pad = layer_1_dilation_size*(layer_1_kernel_size-1)\n",
    "        self.temp_conv_layer_1 = nn.Sequential(\n",
    "                        nn.ZeroPad2d((self.layer_1_num_zero_pad, 0, 0, 0)),\n",
    "                        Kerv1d(in_channels = num_of_src_features,\n",
    "                               out_channels = num_of_src_features,\n",
    "                               kernel_size = layer_1_kernel_size,\n",
    "                               stride = 1,\n",
    "                               padding = 0,\n",
    "                               dilation = layer_1_dilation_size,\n",
    "                               kernel_type = 'gaussian', \n",
    "                               learnable_kernel = True).to(device),\n",
    "                        nn.GELU()\n",
    "        )  \n",
    "        \n",
    "        layer_2_dilation_size = 2\n",
    "        layer_2_kernel_size = 2\n",
    "        self.layer_2_num_zero_pad = layer_2_dilation_size*(layer_2_kernel_size-1)\n",
    "        self.temp_conv_layer_2 = nn.Sequential(\n",
    "                        nn.ZeroPad2d((self.layer_2_num_zero_pad, 0, 0, 0)),\n",
    "                        Kerv1d(in_channels = 2*num_of_src_features,\n",
    "                               out_channels = 2*num_of_src_features,\n",
    "                               kernel_size = layer_2_kernel_size,\n",
    "                               stride = 1,\n",
    "                               padding = 0,\n",
    "                               dilation = layer_2_dilation_size,kernel_type = 'gaussian', \n",
    "                               learnable_kernel = True).to(device),\n",
    "                        nn.GELU()\n",
    "        )         \n",
    "        \n",
    "        layer_3_dilation_size = 4\n",
    "        layer_3_kernel_size = 2\n",
    "        self.layer_3_num_zero_pad = layer_3_dilation_size*(layer_3_kernel_size-1)\n",
    "        self.temp_conv_layer_3 = nn.Sequential(\n",
    "                        nn.ZeroPad2d((self.layer_3_num_zero_pad, 0, 0, 0)),\n",
    "                        Kerv1d(in_channels = 4*num_of_src_features,\n",
    "                               out_channels = 4*num_of_src_features,\n",
    "                               kernel_size = layer_3_kernel_size,\n",
    "                               stride = 1,\n",
    "                               padding = 0,\n",
    "                               dilation = layer_3_dilation_size,kernel_type = 'gaussian', \n",
    "                               learnable_kernel = True).to(device),\n",
    "                        nn.GELU()\n",
    "                        )       \n",
    "        layer_4_dilation_size = 8\n",
    "        layer_4_kernel_size = 2\n",
    "        self.layer_4_num_zero_pad = layer_4_dilation_size*(layer_4_kernel_size-1)\n",
    "        self.temp_conv_layer_4 = nn.Sequential(\n",
    "                        nn.ZeroPad2d((self.layer_4_num_zero_pad, 0, 0, 0)),\n",
    "                        Kerv1d(in_channels = 8*num_of_src_features,\n",
    "                               out_channels = 8*num_of_src_features,\n",
    "                               kernel_size = layer_4_kernel_size,\n",
    "                               stride = 1,\n",
    "                               padding = 0,\n",
    "                               dilation = layer_4_dilation_size,kernel_type = 'gaussian', \n",
    "                               learnable_kernel = True).to(device),\n",
    "                        nn.GELU()            \n",
    "                        )    \n",
    "    \n",
    "\n",
    "        \n",
    "        self.fully_connected = nn.Sequential(\n",
    "                        nn.Linear(16*num_of_src_features, dim_model)\n",
    "                        )   \n",
    "        \n",
    "    def forward(self, src): \n",
    "        \n",
    "        input_seq = src.permute(0,2,1) # out_size: batch, features, seq_len    \n",
    "        #print(input_seq.shape)\n",
    "        output_1 = self.temp_conv_layer_1(input_seq)  # out_size: batch, feature, seq_len\n",
    "           \n",
    "        input_seq = input_seq + output_1 # residual connection\n",
    "        input_seq = torch.cat((input_seq, output_1), dim=1) \n",
    "        #print(input_seq.shape,output_1.shape)\n",
    "        \n",
    "        output_2 = self.temp_conv_layer_2(input_seq)\n",
    "        \n",
    "        input_seq = input_seq + output_2 # residual connection\n",
    "        input_seq = torch.cat((input_seq, output_2), dim=1)\n",
    "        #print(input_seq.shape,output_2.shape)\n",
    "        \n",
    "        output_3 = self.temp_conv_layer_3(input_seq)\n",
    "        \n",
    "        input_seq = input_seq + output_3 # residual connection\n",
    "        input_seq = torch.cat((input_seq, output_3), dim=1)\n",
    "        #print(input_seq.shape,output_3.shape)\n",
    "        \n",
    "        output_4 = self.temp_conv_layer_4(input_seq)\n",
    "        \n",
    "        input_seq = input_seq + output_4 # residual connection\n",
    "        input_seq = torch.cat((input_seq, output_4), dim=1)\n",
    "        #print(input_seq.shape,output_4.shape)\n",
    "        \n",
    "        #output_5 = self.temp_conv_layer_5(input_seq)\n",
    "        \n",
    "        #input_seq = input_seq + output_5 # residual connection\n",
    "        #input_seq = torch.cat((input_seq, output_5), dim=1)\n",
    "        #print(input_seq.shape,output_5.shape)\n",
    "        \n",
    "        #input_seq = self.GeLU(input_seq)\n",
    "\n",
    "        output = input_seq.permute(0,2,1) # out_size: batch,  seq_len, feature\n",
    "        #print(input_seq.shape,output.shape) \n",
    "        #print('hello')\n",
    "        #output = torch.flatten(output, 1)\n",
    "\n",
    "        output = self.fully_connected(output)\n",
    "        #print(input_seq.shape,output.shape)\n",
    "        return output           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6b72fa",
   "metadata": {},
   "source": [
    "## Transformer_KCTNencoders with the TF based decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c326b6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SCTransformer_encoder_decoder(nn.Module):\n",
    "\n",
    "    # Constructor\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_model,\n",
    "        len_encoder,\n",
    "        dim_model_O_vehicle,\n",
    "        dim_model_T_vehicle,\n",
    "        num_heads,\n",
    "        num_O_vehicle_heads,\n",
    "        num_T_vehicle_heads,\n",
    "        num_encoder_layers,\n",
    "        num_encoder_layers_o_vehicle,\n",
    "        num_encoder_layers_T_vehicle,\n",
    "        num_decoder_layers,\n",
    "        dropout_p,\n",
    "        num_of_src_features,\n",
    "        num_of_tgt_features,\n",
    "        num_of_src_features_O_vehic=2,\n",
    "        num_of_src_features_T_vehic=5\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # INFO\n",
    "        self.model_type = \"SCTransformer_encoder_decoder\"\n",
    "        self.dim_model = dim_model\n",
    "        self.num_of_src_features = num_of_src_features\n",
    "        self.num_of_tgt_features = num_of_tgt_features\n",
    "        \n",
    "        #self.KCTNT = nn.Linear(  num_of_src_features_T_vehic,dim_model_T_vehicle)\n",
    "        self.KCTNT = KTCN_encoder( dim_model_T_vehicle, num_of_src_features_T_vehic)\n",
    "        self.KCTN1 = KTCN_encoder( dim_model_O_vehicle, num_of_src_features_O_vehic)\n",
    "        self.KCTN2 = KTCN_encoder( dim_model_O_vehicle, num_of_src_features_O_vehic)\n",
    "        self.KCTN3 = KTCN_encoder( dim_model_O_vehicle, num_of_src_features_O_vehic)\n",
    "        self.KCTN4 = KTCN_encoder( dim_model_O_vehicle, num_of_src_features_O_vehic)\n",
    "        self.KCTN5 = KTCN_encoder( dim_model_O_vehicle, num_of_src_features_O_vehic)\n",
    "        self.KCTN6 = KTCN_encoder( dim_model_O_vehicle, num_of_src_features_O_vehic)\n",
    "        \n",
    "        #self.multihead_anT = nn.MultiheadAttention(dim_model_T_vehicle, num_T_vehicle_heads,bias=True)\n",
    "        self.multihead_an1 = nn.MultiheadAttention(dim_model_O_vehicle, num_O_vehicle_heads,bias=True)\n",
    "        self.multihead_an2 = nn.MultiheadAttention(dim_model_O_vehicle, num_O_vehicle_heads,bias=True)\n",
    "        self.multihead_an3 = nn.MultiheadAttention(dim_model_O_vehicle, num_O_vehicle_heads,bias=True)\n",
    "        self.multihead_an4 = nn.MultiheadAttention(dim_model_O_vehicle, num_O_vehicle_heads,bias=True)\n",
    "        self.multihead_an5 = nn.MultiheadAttention(dim_model_O_vehicle, num_O_vehicle_heads,bias=True)\n",
    "        self.multihead_an6 = nn.MultiheadAttention(dim_model_O_vehicle, num_O_vehicle_heads,bias=True)\n",
    "        \n",
    "       \n",
    "        \n",
    "        #self.MHA_encoders_combine = nn.Linear(5+(2*dim_model_T_vehicle)+(6*dim_model_O_vehicle), dim_model)\n",
    "        #self.Linear1= nn.Linear(dim_model_T_vehicle+(6*dim_model_O_vehicle), dim_model)         \n",
    "        #self.Linear2= nn.Linear(dim_model_T_vehicle+(6*dim_model_O_vehicle), dim_model)      \n",
    "        self.Linear1= nn.Linear(6*dim_model_O_vehicle, dim_model)         \n",
    "        self.Linear2= nn.Linear(6*dim_model_O_vehicle, dim_model)    \n",
    "        self.Linear3= nn.Linear(dim_model_T_vehicle, dim_model)        \n",
    "        #self.embedding_encoder = nn.Linear(num_of_src_features, dim_model)\n",
    "\n",
    "        self.multihead_an = nn.MultiheadAttention(dim_model, num_heads,bias=True)      \n",
    "        #self.encoder_layer = nn.TransformerEncoderLayer(d_model=dim_model, nhead=num_heads, dropout=dropout_p)    \n",
    "        #self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_encoder_layers)  \n",
    "        self.Linear4= nn.Linear(2*dim_model, dim_model)  \n",
    "        \n",
    "\n",
    "        self.embedding_decoder = nn.Linear(2, dim_model)\n",
    "        self.embedding_for_pridectedbehavior = nn.Linear(6, dim_model)\n",
    "        self.embedding = nn.Linear(8, dim_model)\n",
    "        \n",
    "        self.decoder1 = nn.Linear(dim_model*len_encoder,30) \n",
    "        self.dropoutlayer1=nn.Dropout(p=dropout_p)\n",
    "        self.decoder11 = nn.Linear(30,3)        \n",
    "        self.decoder2 = nn.Linear(dim_model*len_encoder,30) \n",
    "        self.dropoutlayer2=nn.Dropout(p=dropout_p)\n",
    "        self.decoder22 = nn.Linear(30,3)  \n",
    "\n",
    "        \n",
    "        self.decoder_layer = nn.TransformerDecoderLayer(d_model=dim_model, nhead=num_heads, dropout=dropout_p)    \n",
    "        self.transformer_decoder = nn.TransformerDecoder(self.decoder_layer, num_layers=num_decoder_layers)  \n",
    "        self.decoder3 = nn.Linear(dim_model,num_of_src_features_O_vehic)  \n",
    "    \n",
    "    def forward(self, src,tgt,tgt_mask=None, src_pad_mask=None, tgt_pad_mask=None): \n",
    "        #print(src.shape,tgt.shape)\n",
    "        #srcT=self.KCTNT(src[:,:,0:5])          \n",
    "        src1=self.KCTN1(src[:,:,[1,8]])\n",
    "        src2=self.KCTN2(src[:,:,[2,9]])\n",
    "        src3=self.KCTN3(src[:,:,[3,10]])\n",
    "        src4=self.KCTN4(src[:,:,[4,11]])\n",
    "        src5=self.KCTN5(src[:,:,[5,12]])\n",
    "        src6=self.KCTN6(src[:,:,[6,13]])        \n",
    "        srcT=self.KCTNT(src[:,:,[0,7,14,15,16]])  \n",
    "        ##attn_outputT, attn_output_weightsT = self.multihead_anT(srcT.permute(1,0,2),srcT.permute(1,0,2),srcT.permute(1,0,2))\n",
    "        \n",
    "        attn_output1, attn_output_weights1 = self.multihead_an1(srcT.permute(1,0,2),src1.permute(1,0,2),src1.permute(1,0,2))\n",
    "        attn_output2, attn_output_weights2 = self.multihead_an2(srcT.permute(1,0,2),src2.permute(1,0,2),src2.permute(1,0,2))\n",
    "        attn_output3, attn_output_weights3 = self.multihead_an3(srcT.permute(1,0,2),src3.permute(1,0,2),src3.permute(1,0,2))\n",
    "        attn_output4, attn_output_weights4 = self.multihead_an4(srcT.permute(1,0,2),src4.permute(1,0,2),src4.permute(1,0,2))\n",
    "        attn_output5, attn_output_weights5 = self.multihead_an5(srcT.permute(1,0,2),src5.permute(1,0,2),src5.permute(1,0,2))\n",
    "        attn_output6, attn_output_weights6 = self.multihead_an6(srcT.permute(1,0,2),src6.permute(1,0,2),src6.permute(1,0,2))                \n",
    "        \n",
    "        #Target_V_data=src[:,:,0:5]\n",
    "        attn_outputT = srcT.permute(1,0,2)\n",
    "        #print(attn_outputT.shape)\n",
    "        #src=torch.cat((Target_V_data,srcT,attn_outputT,attn_output1,attn_output2,attn_output3,attn_output4,attn_output5,attn_output6),dim=2)\n",
    "        src=torch.cat((attn_output1,attn_output2,attn_output3,attn_output4,attn_output5,attn_output6),dim=2)        \n",
    "        #src = src.permute(1,0,2)\n",
    "        #print(src.shape,attn_outputT.shape )\n",
    "        V,K=self.Linear1(src),self.Linear2(src)\n",
    "        Q=self.Linear3(attn_outputT)\n",
    "\n",
    "        #print(Q.shape,V.shape)\n",
    "        \n",
    "        attn_output, attn_output_weights= self.multihead_an(Q,K,V)  \n",
    "    \n",
    "        attn_output = torch.cat((attn_output,Q),dim=2).permute(1,0,2)\n",
    "        attn_output=self.Linear4(attn_output)\n",
    "        # Attention making code     \n",
    "        #print(attn_output.shape )\n",
    "\n",
    "                        \n",
    "        transformer_out1 = attn_output\n",
    "        \n",
    "        transformer_out1 = torch.flatten(transformer_out1, 1)\n",
    "        #print(src.shape,tgt.shape,transformer_out1.shape)\n",
    "        transformer_out11 = self.decoder1(transformer_out1)        \n",
    "        transformer_out11=F.relu(self.dropoutlayer1(transformer_out11))\n",
    "        transformer_out11 = self.decoder11(transformer_out11)\n",
    " \n",
    "        transformer_out12 = self.decoder2(transformer_out1)        \n",
    "        transformer_out12=F.relu(self.dropoutlayer2(transformer_out12))\n",
    "        transformer_out12 = self.decoder22(transformer_out12)\n",
    "        \n",
    "        decoder_input=torch.cat((transformer_out11,transformer_out12),dim=1)\n",
    "        \n",
    "        #print(transformer_out11.shape,transformer_out12.shape,decoder_input.shape)\n",
    "        decoder_input=decoder_input.unsqueeze(1)\n",
    "        #print(transformer_out11.shape,transformer_out12.shape,decoder_input.shape)\n",
    "        decoder_input=decoder_input.expand(decoder_input.shape[0],tgt.shape[1],decoder_input.shape[2])\n",
    "        #decoder_input=self.embedding_for_pridectedbehavior(decoder_input)\n",
    "        #print(decoder_input.shape,tgt.shape)\n",
    "        tgt=torch.cat((decoder_input,tgt),dim=2)\n",
    "            \n",
    "        tgt = self.embedding(tgt)  \n",
    "        tgt = tgt.permute(1,0,2)\n",
    "        transformer_out=self.transformer_decoder(tgt, attn_output.permute(1,0,2),tgt_mask)\n",
    "        transformer_out = transformer_out.permute(1,0,2)   \n",
    "        transformer_out = self.decoder3(transformer_out)\n",
    "\n",
    "        return transformer_out11,transformer_out12,transformer_out\n",
    "      \n",
    "    def get_tgt_mask(self, size) -> torch.tensor:\n",
    "        mask = torch.tril(torch.ones(size, size) == 1) # Lower triangular matrix\n",
    "        mask = mask.float()\n",
    "        mask = mask.masked_fill(mask == 0, float('-inf')) # Convert zeros to -inf\n",
    "        mask = mask.masked_fill(mask == 1, float(0.0)) # Convert ones to 0\n",
    "        \n",
    "        return mask\n",
    "    \n",
    "    def create_pad_mask(self, matrix: torch.tensor, pad_token: int) -> torch.tensor:\n",
    "        return (matrix == pad_token)       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "31bf42c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_model = 64\n",
    "num_heads = 8\n",
    "num_encoder_layers = 1\n",
    "num_decoder_layers = 1\n",
    "dropout_p = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "203a8db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_model_O_vehicle=64\n",
    "dim_model_T_vehicle=64\n",
    "\n",
    "num_O_vehicle_heads=8\n",
    "num_T_vehicle_heads=8\n",
    "\n",
    "num_encoder_layers_o_vehicle=1\n",
    "num_encoder_layers_T_vehicle=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e741b1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device=\"cpu\"\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bbb01dba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "99f3d1c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "246eafbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_model_2 = SCTransformer_encoder_decoder(dim_model = dim_model,\n",
    "                                           len_encoder=encoder_length,\n",
    "                                           dim_model_O_vehicle = dim_model_O_vehicle,\n",
    "                                           dim_model_T_vehicle = dim_model_T_vehicle,\n",
    "                                           num_heads = num_heads,\n",
    "                                           num_O_vehicle_heads=num_O_vehicle_heads,\n",
    "                                           num_T_vehicle_heads=num_T_vehicle_heads,\n",
    "                                           num_encoder_layers = num_encoder_layers,\n",
    "                                           num_encoder_layers_o_vehicle=num_encoder_layers_o_vehicle,\n",
    "                                           num_encoder_layers_T_vehicle=num_encoder_layers_T_vehicle,\n",
    "                                           num_decoder_layers = num_decoder_layers,\n",
    "                                           dropout_p = dropout_p,\n",
    "                                           num_of_src_features = num_of_src_features,\n",
    "                                           num_of_tgt_features = num_of_tgt_features,\n",
    "                                           num_of_src_features_O_vehic=2,\n",
    "                                           num_of_src_features_T_vehic=5).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5ba0566d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_Best_model2='Best_model_HSTA-KTCN-TF.pt'\n",
    "file_name_Best_model2_test='HSTA-KTCN-TF_test_loss.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f1f2a3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.00005\n",
    "learning_rate=learning_rate*0.95\n",
    "opt = torch.optim.AdamW(tf_model_2.parameters(), lr = learning_rate)\n",
    "loss_fn = AllLoss()\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(opt, 5.0, gamma=0.8)\n",
    "teacher_forcing_ratio=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b329045-b860-4a07-b6ab-ea3ee6257052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for plotting later on    \n",
    "print(\"Training and validating model\")\n",
    "for epoch in range(0,100):\n",
    "    teacher_forcing_ratio=teacher_forcing_ratio-0.02\n",
    "    print(teacher_forcing_ratio)\n",
    "    \n",
    "    a = train_loop_new(epoch,tf_model_2,opt,loss_fn1,loss_fn,train_dataloader,teacher_forcing_ratio = teacher_forcing_ratio)   \n",
    "    final_validation_loss = final_validation_loop(tf_model_2,loss_fn1, loss_fn,val_dataloader,max_length)\n",
    "    print(final_validation_loss[-1,0])\n",
    "\n",
    "torch.save(tf_model_2.state_dict(),file_name_Best_model2) \n",
    "#pd.DataFrame(final_validation_loss).to_csv(file_name_Best_model2_test)                 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1a54e5-b4c4-4a04-92fc-303bca5f2a8c",
   "metadata": {},
   "source": [
    "## testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f599497f-3bee-47ea-8e10-c2cc042c3f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_model_2.load_state_dict(torch.load(file_name_Best_model2))\n",
    "learning_rate=0.00005\n",
    "learning_rate=learning_rate*0.95\n",
    "opt = torch.optim.AdamW(tf_model_2.parameters(), lr = learning_rate)\n",
    "loss_fn = AllLoss()\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(opt, 1.0, gamma=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d32368-9bee-4860-a247-ee6b1ca6d202",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_validation_loss = final_validation_loop(tf_model_2,loss_fn1, loss_fn,val_dataloader,max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e79b1a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(final_validation_loss).to_csv(file_name_Best_model2_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fa0188f2-e6a3-4214-b395-655944f0179c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HSTA-KTCN-TF_test_loss.csv'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name_Best_model2_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47ab4a3-52b4-4857-944c-a13838e00e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE_loss=final_validation_loss[[4::5],0]\n",
    "RMSE_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0df2121-6dec-4e53-b288-37175cf7b5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAE_loss=final_validation_loss[[4::5],2]\n",
    "MAE_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
